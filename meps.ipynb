{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruijiang/.conda/envs/conformalcc/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is  cuda:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os \n",
    "\n",
    "import string \n",
    "from scipy.stats import norm\n",
    "import torch \n",
    "#import torchsort \n",
    "import copy \n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.distributions as dist\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from dataset_prepare import * \n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "from probaforms.models import CVAE\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--n', type=int, default=2000)\n",
    "parser.add_argument('--d', type=int, default=1)\n",
    "parser.add_argument('--nval', type=int, default=1000)\n",
    "parser.add_argument('--alpha', type=float, default=0.1)\n",
    "parser.add_argument('--niter', type=int, default=100)\n",
    "parser.add_argument('--densitymodel', type=str, default='CVAE')\n",
    "parser.add_argument('--dataset', type=str, default='meps_19')\n",
    "parser.add_argument('--lamb', type=float, default=100)\n",
    "parser.add_argument('--model', type=str, default='linear')\n",
    "parser.add_argument('--conformalscore', type=str, default='residual')\n",
    "parser.add_argument('--wsc_delta', type=float, default=0.1)\n",
    "\n",
    "args = parser.parse_args([])\n",
    "n = args.n\n",
    "d = args.d\n",
    "alpha = args.alpha\n",
    "niter = args.niter\n",
    "model = args.model\n",
    "densitymodel = args.densitymodel\n",
    "dataset_name = args.dataset\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = 'cpu'\n",
    "print('device is ', device)\n",
    "\n",
    "def conformalScore(Y, Yhat, sd = 1):\n",
    "    if args.conformalscore == 'residual':\n",
    "        score = np.abs(Yhat - Y)\n",
    "    elif args.conformalscore == 'normalized':\n",
    "        score = np.abs(Yhat - Y) / sd\n",
    "    return score\n",
    "\n",
    "def conformalScore_torch(Y, Yhat, sd = 1):\n",
    "    if args.conformalscore == 'residual':\n",
    "        score = torch.abs(Yhat - Y)\n",
    "    elif args.conformalscore == 'normalized':\n",
    "        score = torch.abs(Yhat - Y) / sd\n",
    "    return score\n",
    "\n",
    "\n",
    "outfun = LinearRegression()\n",
    "\n",
    "# torch linear model \n",
    "class LinearModel(torch.nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(d, 1)\n",
    "        torch.nn.init.xavier_uniform_(self.linear.weight)\n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "    \n",
    "\n",
    "# a MLP with LeakyReLU activation\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super(MLP, self).__init__()\n",
    "        hidd = 16\n",
    "        self.linear1 = torch.nn.Linear(d, hidd)\n",
    "        self.linear2 = torch.nn.Linear(hidd, hidd)\n",
    "        self.linear3 = torch.nn.Linear(hidd, 1)\n",
    "        self.leakyrelu = torch.nn.LeakyReLU()\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        # initialize the weights\n",
    "        torch.nn.init.xavier_uniform_(self.linear1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.linear2.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.linear3.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear1(x)\n",
    "        y_pred = self.leakyrelu(y_pred)\n",
    "        y_pred = self.linear2(y_pred)\n",
    "        y_pred = self.leakyrelu(y_pred)\n",
    "        y_pred = self.linear3(y_pred)\n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "# a class for the torch linear model with fit and predict methods\n",
    "class TorchBaseModel():\n",
    "    def __init__(self, d):\n",
    "        if args.model == 'linear' or args.model == 'linear_cc':\n",
    "            self.model = LinearModel(d).to(device)\n",
    "        elif args.model == 'mlp' or args.model == 'mlp_cc':\n",
    "            self.model = MLP(d).to(device)\n",
    "        self.criterion = torch.nn.MSELoss(reduction='mean')\n",
    "        #self.optimizer = torch.optim.SGD(self.model.parameters(), lr=1e-3)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        X = torch.from_numpy(X).float().to(device)\n",
    "        Y = torch.from_numpy(Y).float().to(device)\n",
    "        for t in range(10000):\n",
    "            # Forward pass: Compute predicted y by passing x to the model\n",
    "            y_pred = self.model(X)\n",
    "            # Compute and print loss\n",
    "            loss = ((y_pred.reshape(-1) - Y)**2).mean()\n",
    "            # Zero gradients, perform a backward pass, and update the weights.\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            if t % 100 == 99:\n",
    "                print(t, loss.item())\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = torch.from_numpy(X).float().to(device)\n",
    "        return self.model(X).detach().cpu().numpy().reshape(-1)\n",
    "\n",
    "class TorchLinearModel_CC():\n",
    "    def __init__(self, d, lamb = 1, density_model = None):\n",
    "        if args.model == 'linear_cc':\n",
    "            self.model = LinearModel(d).to(device)\n",
    "        elif args.model == 'mlp_cc':\n",
    "            self.model = MLP(d).to(device)\n",
    "        self.lamb = lamb\n",
    "        self.criterion = torch.nn.MSELoss(reduction='mean')\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        self.density_model = density_model\n",
    "        \n",
    "    def fit(self, X, Y, X_CC, Y_CC, Xtest, Ytest, sen, i = 0):\n",
    "        # plot the data\n",
    "        X = torch.from_numpy(X).float().to(device)\n",
    "        Y = torch.from_numpy(Y).float().to(device)\n",
    "        Xval = X_CC\n",
    "        Yval = Y_CC\n",
    "        X_CC = torch.from_numpy(X_CC).float().to(device)\n",
    "        Y_CC = torch.from_numpy(Y_CC).float().to(device)\n",
    "        X_CC = X\n",
    "        Y_CC = Y\n",
    "        \n",
    "        #first train a simple linear model\n",
    "        for t in range(10000):\n",
    "            # Forward pass: Compute predicted y by passing x to the model\n",
    "            y_pred = self.model(X)\n",
    "            # Compute and print loss\n",
    "            loss = ((y_pred.reshape(-1) - Y)**2).mean()\n",
    "            \n",
    "            # Zero gradients, perform a backward pass, and update the weights.\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        nsamples = 2000\n",
    "        samples = np.zeros((X_CC.shape[0], nsamples))\n",
    "        for i in range(nsamples):\n",
    "            condX = X_CC.cpu().numpy()\n",
    "            samples[:,i] = self.density_model.sample(condX).reshape(-1)\n",
    "        sd_cc = np.std(samples, axis = 1)\n",
    "        sd_cc = torch.from_numpy(sd_cc).float().to(device)\n",
    "\n",
    "        samples_test = np.zeros((Xtest.shape[0], nsamples))\n",
    "        for i in range(nsamples):\n",
    "            condX = Xtest\n",
    "            samples_test[:,i] = self.density_model.sample(condX).reshape(-1)\n",
    "        sd_test = np.std(samples_test, axis = 1)\n",
    "        sd_test = torch.from_numpy(sd_test).float().to(device)\n",
    "\n",
    "        best_loss = 1e7\n",
    "        improved_in_last = 0\n",
    "        for t in range(60):\n",
    "            # train a density model \n",
    "            y_pred = self.model(X_CC)\n",
    "            V_CC = conformalScore_torch(Y_CC, y_pred.reshape(-1), sd = sd_cc)\n",
    "            \n",
    "            y_pred = self.model(X)\n",
    "            loss = ((y_pred.reshape(-1) - Y)**2).mean()\n",
    "            \n",
    "            y_pred = self.model(X_CC)\n",
    "            V_CC = conformalScore_torch(Y_CC, y_pred.reshape(-1), sd = sd_cc)\n",
    "            \n",
    "            wdiv = 0 \n",
    "            nsamples = 500\n",
    "            nsamples = min(nsamples, X_CC.shape[0])\n",
    "            batch_size = 100\n",
    "            temperature = 10\n",
    "            # random sample nsamples from X_CC\n",
    "            rand_index = np.random.choice(X_CC.shape[0], nsamples)\n",
    "            wdiv = torch.zeros_like(V_CC[:nsamples])\n",
    "            for indi, i in enumerate(rand_index):\n",
    "                if densitymodel == 'mdn':\n",
    "                    thissampled = torch.Tensor(self.density_model.sample((torch.ones_like(X_CC) * X_CC[i,])[:nsamples].cpu().numpy())[1].reshape(-1))\n",
    "                else:\n",
    "                    condX = (torch.ones_like(X_CC) * X_CC[i,])[:nsamples].cpu().numpy()\n",
    "                    #thissampled = self.density_model.generate(condX.shape[0], cond = condX).values.reshape(-1)\n",
    "                    thissampled = self.density_model.sample(condX).reshape(-1)\n",
    "                    thissampled = torch.Tensor(thissampled)\n",
    "                thissampled = thissampled.to(device)\n",
    "                sample_index = np.random.choice(X_CC.shape[0], nsamples)\n",
    "                VCC_sampled = V_CC[sample_index]\n",
    "                V_givenx = conformalScore_torch(thissampled, torch.ones_like(y_pred.reshape(-1)[:nsamples]) * y_pred[i,], \\\n",
    "                                sd = torch.ones_like(y_pred.reshape(-1)[:nsamples]) * sd_cc[i])\n",
    "\n",
    "                diff = approx_ecdf(V_givenx, VCC_sampled, grid_size = 100, temperature = 10)\n",
    "                # when the max cannot be reduced effectively, minimize the softmax is still effective for improving the cc\n",
    "                wdiv[indi] = (diff * F.softmax(diff * temperature, dim = 0)).sum()\n",
    "\n",
    "            div = wdiv * F.softmax(wdiv * temperature, dim = 0)\n",
    "            div = div.sum()\n",
    "            loss1 = loss + self.lamb * div.mean()\n",
    "            print('t = ', t, 'loss = ', loss, 'div = ', div, 'loss1 = ', loss1)\n",
    "\n",
    "            # Zero gradients, perform a backward pass, and update the weights.\n",
    "            self.optimizer.zero_grad()\n",
    "            loss1.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if loss1 < best_loss:\n",
    "                best_loss = loss1\n",
    "                best_model = copy.deepcopy(self.model)\n",
    "                improved_in_last = 0\n",
    "            \n",
    "            if (t+1) % 20 == 0:\n",
    "                alpha = 0.1 \n",
    "                Yhat = self.predict(Xval)\n",
    "                Yscore = conformalScore(Yval, Yhat, sd = sd_cc.cpu().numpy())\n",
    "\n",
    "                Yhat_test = self.predict(Xtest)\n",
    "                \n",
    "                nval = Xval.shape[0]\n",
    "                qhat = np.quantile(Yscore, np.ceil((nval+1)*(1-alpha))/nval, interpolation='higher')\n",
    "                Yslack = qhat\n",
    "\n",
    "                print(f'cutoff is {Yslack}')\n",
    "                if args.conformalscore == 'residual':\n",
    "                    Ylo = Yhat_test - Yslack\n",
    "                    Yup = Yhat_test + Yslack\n",
    "                elif args.conformalscore == 'normalized':\n",
    "                    Ylo = Yhat_test - Yslack * sd_test.cpu().numpy()\n",
    "                    Yup = Yhat_test + Yslack * sd_test.cpu().numpy()\n",
    "\n",
    "                CI = pd.DataFrame({\"lower\": Ylo.reshape(-1), \"upper\": Yup.reshape(-1)})\n",
    "\n",
    "                cover = (Ylo <= Ytest) & (Ytest <= Yup)\n",
    "                cc = 100\n",
    "                for eachs in set(sen):\n",
    "                    cc = min(cc, cover[sen == eachs].mean())\n",
    "\n",
    "                pred = np.concatenate([Ylo.reshape(-1,1), Yup.reshape(-1,1)], axis = 1)\n",
    "                wslab = wsc_unbiased(Xtest, Ytest, pred, delta=args.wsc_delta, M=max(200,Xtest.shape[1]+50), test_size=0.5, random_state=2020, verbose=False)\n",
    "                print(t, loss.item())\n",
    "                print(f'mc is {np.mean((Ylo <= Ytest) & (Ytest <= Yup))}')\n",
    "                print(f'cc is {cc}')\n",
    "                print(f'wsc is {wslab}')\n",
    "        \n",
    "        y_pred = self.model(X_CC).reshape(-1)\n",
    "        V_CC = conformalScore_torch(Y_CC, y_pred)\n",
    "        # plot V_CC\n",
    "        self.model = best_model\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = torch.from_numpy(X).float().to(device)\n",
    "        return self.model(X).detach().cpu().numpy().reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random \n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "def wsc(X, y, pred, delta=0.1, M=1000, verbose=False):\n",
    "    # Extract lower and upper prediction bands\n",
    "    pred_l = np.min(pred,1)\n",
    "    pred_h = np.max(pred,1)\n",
    "\n",
    "    def wsc_v(X, y, pred_l, pred_h, delta, v):\n",
    "        n = len(y)\n",
    "        cover = (y>=pred_l)*(y<=pred_h)\n",
    "        z = np.dot(X,v)\n",
    "        # Compute mass\n",
    "        z_order = np.argsort(z)\n",
    "        z_sorted = z[z_order]\n",
    "        cover_ordered = cover[z_order]\n",
    "        ai_max = int(np.round((1.0-delta)*n))\n",
    "        ai_best = 0\n",
    "        bi_best = n-1\n",
    "        cover_min = np.mean((y >= pred_l)*(y <= pred_h))\n",
    "        for ai in np.arange(0, ai_max):\n",
    "            bi_min = np.minimum(ai+int(np.round(delta*n)),n)\n",
    "            coverage = np.cumsum(cover_ordered[ai:n]) / np.arange(1,n-ai+1)\n",
    "            coverage[np.arange(0,bi_min-ai)]=1\n",
    "            bi_star = ai+np.argmin(coverage)\n",
    "            cover_star = coverage[bi_star-ai]\n",
    "            if cover_star < cover_min:\n",
    "                ai_best = ai\n",
    "                bi_best = bi_star\n",
    "                cover_min = cover_star\n",
    "        return cover_min, z_sorted[ai_best], z_sorted[bi_best]\n",
    "\n",
    "    def sample_sphere(n, p):\n",
    "        v = np.random.randn(p, n)\n",
    "        v /= np.linalg.norm(v, axis=0)\n",
    "        v = v.T\n",
    "        return v\n",
    "\n",
    "    seed_everything(2020)\n",
    "    V = sample_sphere(M, p=X.shape[1])\n",
    "    wsc_list = [[]] * M\n",
    "    a_list = [[]] * M\n",
    "    b_list = [[]] * M\n",
    "    if verbose:\n",
    "        for m in tqdm(range(M)):\n",
    "            wsc_list[m], a_list[m], b_list[m] = wsc_v(X, y, pred_l, pred_h, delta, V[m])\n",
    "    else:\n",
    "        for m in range(M):\n",
    "            wsc_list[m], a_list[m], b_list[m] = wsc_v(X, y, pred_l, pred_h, delta, V[m])\n",
    "        \n",
    "    idx_star = np.argmin(np.array(wsc_list))\n",
    "    a_star = a_list[idx_star]\n",
    "    b_star = b_list[idx_star]\n",
    "    v_star = V[idx_star]\n",
    "    wsc_star = wsc_list[idx_star]\n",
    "    return wsc_star, v_star, a_star, b_star\n",
    "\n",
    "def wsc_unbiased(X, y, pred, delta=0.1, M=1000, test_size=0.75, random_state=2020, verbose=False):\n",
    "    test_size = 0.75\n",
    "    M = 1000\n",
    "    def wsc_vab(X, y, pred, v, a, b):\n",
    "        # Extract lower and upper prediction bands\n",
    "        pred_l = np.min(pred,1)\n",
    "        pred_h = np.max(pred,1)\n",
    "        n = len(y)\n",
    "        cover = (y>=pred_l)*(y<=pred_h)\n",
    "        z = np.dot(X,v)\n",
    "        idx = np.where((z>=a)*(z<=b))\n",
    "        coverage = np.mean(cover[idx])\n",
    "        return coverage\n",
    "\n",
    "    seed_everything(random_state)\n",
    "    X_train, X_test, y_train, y_test, pred_train, pred_test = train_test_split(X, y, pred, test_size=test_size,\n",
    "                                                                         random_state=random_state)\n",
    "    # Find adversarial parameters\n",
    "    wsc_star, v_star, a_star, b_star = wsc(X_train, y_train, pred_train, delta=delta, M=M, verbose=verbose)\n",
    "    # Estimate coverage\n",
    "    coverage = wsc_vab(X_test, y_test, pred_test, v_star, a_star, b_star)\n",
    "    return coverage\n",
    "\n",
    "\n",
    "import random \n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "def approx_ecdf(points1, points2, grid_size = 100, temperature = 5):\n",
    "    tgrid = torch.linspace(min(min(points1), min(points2)).item()-0.1, max(max(points1),max(points2)).item()+0.1, grid_size)\n",
    "    tgrid = tgrid.to(device)\n",
    "    cdf1 = (1-torch.sigmoid(temperature * (points1.reshape(-1,1) - tgrid))).mean(0)\n",
    "    cdf2 = (1-torch.sigmoid(temperature * (points2.reshape(-1,1) - tgrid))).mean(0)\n",
    "    diff = torch.abs(cdf1 - cdf2)\n",
    "    return diff \n",
    "\n",
    "coverage = []\n",
    "conditional_coverage = []\n",
    "set_size = []\n",
    "alpha_grid = np.linspace(0.1, 0.9, 10)\n",
    "results = pd.DataFrame({\"iter\": [], \"alpha\": [], \"coverage\": [], \"set_size\": [], \"conditional_coverage\": [], 'mse': [], 'wslab': []})\n",
    "mdn_results = pd.DataFrame({\"iter\": [], \"alpha\": [], \"coverage\": [], \"set_size\": [], \"conditional_coverage\": [], 'mse': [], 'wslab': []})\n",
    "base_results = pd.DataFrame({\"iter\": [], \"alpha\": [], \"coverage\": [], \"set_size\": [], \"conditional_coverage\": [], 'mse': [], 'wslab': []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size is 10428, number of features is 139.\n",
      "train size is 6256, val size is 2085, test size is 2087.\n",
      "density model fitting done.\n"
     ]
    }
   ],
   "source": [
    "# mlp model \n",
    "seed_everything(0)\n",
    "X, Y = GetDataset(dataset_name, './data/')\n",
    "Y += 1e-3 * np.random.normal(size=Y.shape)\n",
    "\n",
    "basemodel = TorchBaseModel(X.shape[1])\n",
    "\n",
    "index = np.random.permutation(X.shape[0])\n",
    "print(f'dataset size is {X.shape[0]}, number of features is {X.shape[1]}.')\n",
    "nval = int(X.shape[0] * 0.2)\n",
    "ntrain = int(X.shape[0] * 0.6)\n",
    "Xval = X[index[:nval],:]\n",
    "Yval = Y[index[:nval]]\n",
    "Xtest = X[index[(nval+ntrain):],:]\n",
    "Ytest = Y[index[(nval+ntrain):]]\n",
    "X = X[index[(nval):(nval+ntrain)],:]\n",
    "Y = Y[index[(nval):(nval+ntrain)]]\n",
    "print(f'train size is {X.shape[0]}, val size is {Xval.shape[0]}, test size is {Xtest.shape[0]}.')\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "\n",
    "y_scaler = StandardScaler()\n",
    "y_scaler.fit(Y.reshape(-1,1))\n",
    "\n",
    "Y = y_scaler.transform(Y.reshape(-1,1)).reshape(-1)\n",
    "Yval = y_scaler.transform(Yval.reshape(-1,1)).reshape(-1)\n",
    "Ytest = y_scaler.transform(Ytest.reshape(-1,1)).reshape(-1)\n",
    "\n",
    "if dataset_name == 'meps_19' or dataset_name == 'meps_20' or dataset_name == 'meps_21':\n",
    "    # define a subgroup - checking some surrogate variable\n",
    "    sen = np.zeros_like(Ytest)\n",
    "    sen[(Xtest[:, 9] == 1) & (Xtest[:, -1] == 1)] = 0\n",
    "    sen[(Xtest[:, 9] == 1) & (Xtest[:, -1] == 0)] = 1\n",
    "    sen[(Xtest[:, 9] == 0) & (Xtest[:, -1] == 1)] = 2\n",
    "    sen[(Xtest[:, 9] == 0) & (Xtest[:, -1] == 0)] = 3\n",
    "    \n",
    "X = scaler.transform(X)\n",
    "Xval = scaler.transform(Xval)\n",
    "Xtest = scaler.transform(Xtest)\n",
    "\n",
    "if (model == 'linear' or model == 'mlp') and args.conformalscore == 'residual':\n",
    "    pass \n",
    "elif args.densitymodel == 'cvae':\n",
    "    density_model = CVAE(n_epochs=500, latent_dim=8, hidden = (32,32), batch_size=128, lr=1e-3, weight_decay=1e-4, verbose = 1)\n",
    "    density_model.fit(Y.reshape(-1,1), X)\n",
    "\n",
    "print('density model fitting done.')\n",
    "nval = Xval.shape[0]\n",
    "\n",
    "nsamples = 1000\n",
    "\n",
    "if (model == 'linear' or model == 'mlp') and args.conformalscore == 'residual':\n",
    "    val_std = 1\n",
    "    test_std = 1\n",
    "else:\n",
    "    samples_val = np.zeros((nval, nsamples))\n",
    "    for i in range(nsamples):\n",
    "        #samples_val[:,i] = density_model.sample(Xval)[1].reshape(-1)\n",
    "        #samples_val[:,i] = density_model.generate(Xval.shape[0], cond = Xval).values.reshape(-1)\n",
    "        samples_val[:,i] = density_model.sample(Xval).reshape(-1)\n",
    "\n",
    "    val_std = np.std(samples_val, axis = 1)\n",
    "\n",
    "    ntest = Xtest.shape[0]\n",
    "    samples = np.zeros((ntest, nsamples))\n",
    "    for i in range(nsamples):\n",
    "        #samples[:,i] = density_model.sample(Xtest)[1].reshape(-1)\n",
    "        #samples[:,i] = density_model.generate(Xtest.shape[0], cond = Xtest).values.reshape(-1)\n",
    "        samples[:,i] = density_model.sample(Xtest).reshape(-1)\n",
    "\n",
    "    test_std = np.std(samples, axis = 1)\n",
    "\n",
    "    # evaluate genmodel \n",
    "    print('evaluate genmodel')\n",
    "    for alpha in alpha_grid:\n",
    "        # evaluate density model's conditional/marginal coverage\n",
    "        Y_lo = np.quantile(samples, alpha/2, axis = 1)\n",
    "        Y_up = np.quantile(samples, 1 - alpha/2, axis = 1)\n",
    "\n",
    "        marginal_coverage = np.mean((Y_lo <= Ytest) & (Ytest <= Y_up))\n",
    "        print(f'marginal coverage of density model is {marginal_coverage}')\n",
    "\n",
    "        cover = (Y_lo <= Ytest) & (Ytest <= Y_up)\n",
    "        cc = 1\n",
    "        for eachs in set(sen):\n",
    "            cc = min(cc, cover[sen == eachs].mean())\n",
    "\n",
    "        conditional_coverage.append(cc)\n",
    "        print(f'Average conditional coverage of density model is {cc}')\n",
    "        coverage.append(np.mean((Y_lo <= Ytest) & (Ytest <= Y_up)))\n",
    "        set_size.append(np.mean(Y_up - Y_lo))\n",
    "\n",
    "        pred = np.concatenate([Y_lo.reshape(-1,1), Y_up.reshape(-1,1)], axis = 1)\n",
    "        wslab = wsc_unbiased(Xtest, Ytest, pred, delta=args.wsc_delta, M=max(200,Xtest.shape[1]+50), test_size=0.5, random_state=2020, verbose=False) \n",
    "        print(f'wsc of density model is {wslab}')\n",
    "        est_mean = np.mean(samples, axis = 1)\n",
    "\n",
    "        mdn_results = pd.concat([mdn_results, pd.DataFrame({\"iter\": [iter], \"alpha\": [alpha], \"coverage\": [np.mean((Y_lo <= Ytest) & (Ytest <= Y_up))], \\\n",
    "        \"set_size\": [np.mean(Y_up - Y_lo)], \"conditional_coverage\": [np.min(cc)], 'mse': [((Ytest - est_mean)**2).mean()], \n",
    "        'wslab': [wslab]})])\n",
    "    print('evaluate genmodel done.')\n",
    "\n",
    "    mdn_results.to_csv(f'./log/{args.dataset}_{args.densitymodel}_genmodel_coverage.csv', index=False)\n",
    "    mdn_results_mean = mdn_results.groupby(['alpha']).mean().reset_index()\n",
    "    mdn_results_se = mdn_results.groupby(['alpha']).std().reset_index() / np.sqrt(5)\n",
    "    mdn_results_se.columns = [each + '_se' for each in mdn_results_se.columns]\n",
    "    mdn_results_sum = pd.concat([mdn_results_mean, mdn_results_se], axis = 1)\n",
    "    mdn_results_sum.to_csv(f'./log/{args.dataset}_{args.densitymodel}_genmodel_summary.csv', index=False)\n",
    "\n",
    "    # evaluate basemodel \n",
    "    Ymodel_base = basemodel.fit(X, Y)\n",
    "    Yhat = Ymodel_base.predict(Xval)\n",
    "    Yscore = conformalScore(Yval, Yhat, sd = val_std)\n",
    "    Yhat_test = Ymodel_base.predict(Xtest)\n",
    "    for alpha in alpha_grid:\n",
    "        qhat = np.quantile(Yscore, np.ceil((nval+1)*(1-alpha))/nval, interpolation='higher')\n",
    "        Yslack = qhat\n",
    "\n",
    "        print(f'cutoff is {Yslack}')\n",
    "        if args.conformalscore == 'residual':\n",
    "            Ylo = Yhat_test - Yslack\n",
    "            Yup = Yhat_test + Yslack\n",
    "        elif args.conformalscore == 'normalized':\n",
    "            Ylo = Yhat_test - Yslack * test_std\n",
    "            Yup = Yhat_test + Yslack * test_std\n",
    "\n",
    "        CI = pd.DataFrame({\"lower\": Ylo.reshape(-1), \"upper\": Yup.reshape(-1)})\n",
    "\n",
    "        # check subgroup coverage\n",
    "        cover = (Ylo <= Ytest) & (Ytest <= Yup)\n",
    "        cc = 1\n",
    "        for eachs in set(sen):\n",
    "            cc = min(cc, cover[sen == eachs].mean())\n",
    "\n",
    "        conditional_coverage.append(cc)\n",
    "        print(f'Coverage is {np.mean((Ylo <= Ytest) & (Ytest <= Yup))}')\n",
    "        print(f'Average coverage set size is {np.mean(Yup - Ylo)}')\n",
    "        print(f'Average conditional coverage is {cc}')\n",
    "        coverage.append(np.mean((Ylo <= Ytest) & (Ytest <= Yup)))\n",
    "        set_size.append(np.mean(Yup - Ylo))\n",
    "\n",
    "        pred = np.concatenate([Ylo.reshape(-1,1), Yup.reshape(-1,1)], axis = 1)\n",
    "        wslab = wsc_unbiased(Xtest, Ytest, pred, delta=args.wsc_delta, M=max(200,Xtest.shape[1]+50), test_size=0.5, random_state=2020, verbose=False)\n",
    "        print(f'wsc is {wslab}')\n",
    "\n",
    "        covered_index = ((Ylo <= Ytest) & (Ytest <= Yup))\n",
    "        base_results = pd.concat([base_results, pd.DataFrame({\"iter\": [iter], \"alpha\": [alpha], \"coverage\": [np.mean((Ylo <= Ytest) & (Ytest <= Yup))], \\\n",
    "        \"set_size\": [np.mean(Yup - Ylo)], \"conditional_coverage\": [np.min(cc)], 'mse': [((Ytest - Yhat_test)**2).mean()], \n",
    "        \"wslab\": [wslab]})])\n",
    "\n",
    "        filename = f'./log/{args.dataset}_{args.model}_{args.densitymodel}_{args.conformalscore}_base_coverage.csv'\n",
    "        filename = filename.replace('_cc', '')\n",
    "        base_results.to_csv(filename, index=False)\n",
    "        base_results_mean = base_results.groupby(['alpha']).mean().reset_index()\n",
    "        base_results_se = base_results.groupby(['alpha']).std().reset_index() / np.sqrt(5)\n",
    "        base_results_se.columns = [each + '_se' for each in base_results_se.columns]\n",
    "        base_results_sum = pd.concat([base_results_mean, base_results_se], axis = 1)\n",
    "        filename = f'./log/{args.dataset}_{args.model}_{args.densitymodel}_{args.conformalscore}_base_summary.csv'\n",
    "        filename = filename.replace('_cc', '')\n",
    "        base_results_sum.to_csv(filename, index=False)\n",
    "\n",
    "if model == 'linear_cc':\n",
    "    linearmodel_cc = TorchLinearModel_CC(X.shape[1], lamb=args.lamb, density_model = density_model)\n",
    "    Ymodel = linearmodel_cc.fit(X, Y, Xval, Yval, Xtest, Ytest, sen, i = iter)\n",
    "elif model == 'mlp_cc':\n",
    "    linearmodel_cc = TorchLinearModel_CC(X.shape[1], lamb=args.lamb, density_model = density_model)\n",
    "    Ymodel = linearmodel_cc.fit(X, Y, Xval, Yval, Xtest, Ytest, sen, i = iter)\n",
    "elif model == 'linear' or model == 'mlp':\n",
    "    linearmodel = TorchBaseModel(X.shape[1])\n",
    "    Ymodel = linearmodel.fit(X, Y)\n",
    "\n",
    "Yhat = Ymodel.predict(Xval)\n",
    "Yscore = conformalScore(Yval, Yhat, sd = val_std)\n",
    "Yhat_test = Ymodel.predict(Xtest)\n",
    "\n",
    "alpha = 0.1 \n",
    "qhat = np.quantile(Yscore, np.ceil((nval+1)*(1-alpha))/nval, interpolation='higher')\n",
    "Yslack = qhat\n",
    "\n",
    "print(f'cutoff is {Yslack}')\n",
    "if args.conformalscore == 'residual':\n",
    "    Ylo = Yhat_test - Yslack\n",
    "    Yup = Yhat_test + Yslack\n",
    "elif args.conformalscore == 'normalized':\n",
    "    Ylo = Yhat_test - Yslack * test_std\n",
    "    Yup = Yhat_test + Yslack * test_std\n",
    "\n",
    "CI = pd.DataFrame({\"lower\": Ylo.reshape(-1), \"upper\": Yup.reshape(-1)})\n",
    "\n",
    "# check subgroup coverage\n",
    "cover = (Ylo <= Ytest) & (Ytest <= Yup)\n",
    "cc = 1\n",
    "for eachs in set(sen):\n",
    "    cc = min(cc, cover[sen == eachs].mean())\n",
    "\n",
    "conditional_coverage.append(cc)\n",
    "print(f'Coverage is {np.mean((Ylo <= Ytest) & (Ytest <= Yup))}')\n",
    "print(f'Average coverage set size is {np.mean(Yup - Ylo)}')\n",
    "print(f'Average conditional coverage is {cc}')\n",
    "coverage.append(np.mean((Ylo <= Ytest) & (Ytest <= Yup)))\n",
    "set_size.append(np.mean(Yup - Ylo))\n",
    "\n",
    "pred = np.concatenate([Ylo.reshape(-1,1), Yup.reshape(-1,1)], axis = 1)\n",
    "wslab = wsc_unbiased(Xtest, Ytest, pred, delta=0.1, M=max(200,Xtest.shape[1]+50), test_size=0.5, random_state=2020, verbose=False)\n",
    "print(f'wsc is {wslab}')\n",
    "\n",
    "covered_index = ((Ylo <= Ytest) & (Ytest <= Yup))\n",
    "results = pd.concat([results, pd.DataFrame({\"iter\": [iter], \"alpha\": [alpha], \"coverage\": [np.mean((Ylo <= Ytest) & (Ytest <= Yup))], \\\n",
    "\"set_size\": [np.mean(Yup - Ylo)], \"conditional_coverage\": [np.min(cc)], 'mse': [((Ytest - Yhat_test)**2).mean()], \n",
    "\"wslab\": [wslab]})])\n",
    "\n",
    "results.to_csv(f'./log/{args.dataset}_{args.model}_{args.lamb}_{args.densitymodel}_{args.conformalscore}_coverage.csv', index=False)\n",
    "\n",
    "result_mean = results.groupby(['alpha']).mean().reset_index()\n",
    "result_se = results.groupby(['alpha']).std().reset_index() / np.sqrt(5)\n",
    "result_se.columns = [each + '_se' for each in result_se.columns]\n",
    "result_sum = pd.concat([result_mean, result_se], axis = 1)\n",
    "result_sum.to_csv(f'./log/{args.dataset}_{args.model}_{args.lamb}_{args.densitymodel}_{args.conformalscore}_coverage_summary.csv', index=False)\n",
    "    \n",
    "print(f'WSLAB is {wslab}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size is 10428, number of features is 139.\n",
      "train size is 6256, val size is 2085, test size is 2087.\n",
      "epoch: 0, loss: 35.2357\n",
      "epoch: 10, loss: 1.3820\n"
     ]
    }
   ],
   "source": [
    "# mlp cc model \n",
    "seed_everything(0)\n",
    "X, Y = GetDataset(dataset_name, './data/')\n",
    "Y += 1e-3 * np.random.normal(size=Y.shape)\n",
    "\n",
    "args.model = 'mlp_cc'\n",
    "model = 'mlp_cc'\n",
    "basemodel = TorchBaseModel(X.shape[1])\n",
    "\n",
    "index = np.random.permutation(X.shape[0])\n",
    "print(f'dataset size is {X.shape[0]}, number of features is {X.shape[1]}.')\n",
    "nval = int(X.shape[0] * 0.2)\n",
    "ntrain = int(X.shape[0] * 0.6)\n",
    "Xval = X[index[:nval],:]\n",
    "Yval = Y[index[:nval]]\n",
    "Xtest = X[index[(nval+ntrain):],:]\n",
    "Ytest = Y[index[(nval+ntrain):]]\n",
    "X = X[index[(nval):(nval+ntrain)],:]\n",
    "Y = Y[index[(nval):(nval+ntrain)]]\n",
    "print(f'train size is {X.shape[0]}, val size is {Xval.shape[0]}, test size is {Xtest.shape[0]}.')\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "\n",
    "y_scaler = StandardScaler()\n",
    "y_scaler.fit(Y.reshape(-1,1))\n",
    "\n",
    "Y = y_scaler.transform(Y.reshape(-1,1)).reshape(-1)\n",
    "Yval = y_scaler.transform(Yval.reshape(-1,1)).reshape(-1)\n",
    "Ytest = y_scaler.transform(Ytest.reshape(-1,1)).reshape(-1)\n",
    "\n",
    "if dataset_name == 'meps_19' or dataset_name == 'meps_20' or dataset_name == 'meps_21':\n",
    "    # define a subgroup - checking some surrogate variable\n",
    "    sen = np.zeros_like(Ytest)\n",
    "    sen[(Xtest[:, 9] == 1) & (Xtest[:, -1] == 1)] = 0\n",
    "    sen[(Xtest[:, 9] == 1) & (Xtest[:, -1] == 0)] = 1\n",
    "    sen[(Xtest[:, 9] == 0) & (Xtest[:, -1] == 1)] = 2\n",
    "    sen[(Xtest[:, 9] == 0) & (Xtest[:, -1] == 0)] = 3\n",
    "    \n",
    "X = scaler.transform(X)\n",
    "Xval = scaler.transform(Xval)\n",
    "Xtest = scaler.transform(Xtest)\n",
    "\n",
    "if (model == 'linear' or model == 'mlp') and args.conformalscore == 'residual':\n",
    "    pass \n",
    "elif args.densitymodel == 'CVAE':\n",
    "    density_model = CVAE(n_epochs=500, latent_dim=8, hidden = (32,32), batch_size=128, lr=1e-3, weight_decay=1e-4, verbose = 1)\n",
    "    density_model.fit(Y.reshape(-1,1), X)\n",
    "\n",
    "print('density model fitting done.')\n",
    "nval = Xval.shape[0]\n",
    "\n",
    "nsamples = 1000\n",
    "\n",
    "if (model == 'linear' or model == 'mlp') and args.conformalscore == 'residual':\n",
    "    val_std = 1\n",
    "    test_std = 1\n",
    "else:\n",
    "    samples_val = np.zeros((nval, nsamples))\n",
    "    for i in range(nsamples):\n",
    "        #samples_val[:,i] = density_model.sample(Xval)[1].reshape(-1)\n",
    "        #samples_val[:,i] = density_model.generate(Xval.shape[0], cond = Xval).values.reshape(-1)\n",
    "        samples_val[:,i] = density_model.sample(Xval).reshape(-1)\n",
    "\n",
    "    val_std = np.std(samples_val, axis = 1)\n",
    "\n",
    "    ntest = Xtest.shape[0]\n",
    "    samples = np.zeros((ntest, nsamples))\n",
    "    for i in range(nsamples):\n",
    "        #samples[:,i] = density_model.sample(Xtest)[1].reshape(-1)\n",
    "        #samples[:,i] = density_model.generate(Xtest.shape[0], cond = Xtest).values.reshape(-1)\n",
    "        samples[:,i] = density_model.sample(Xtest).reshape(-1)\n",
    "\n",
    "    test_std = np.std(samples, axis = 1)\n",
    "\n",
    "    # evaluate genmodel \n",
    "    print('evaluate genmodel')\n",
    "    for alpha in alpha_grid:\n",
    "        # evaluate density model's conditional/marginal coverage\n",
    "        Y_lo = np.quantile(samples, alpha/2, axis = 1)\n",
    "        Y_up = np.quantile(samples, 1 - alpha/2, axis = 1)\n",
    "\n",
    "        marginal_coverage = np.mean((Y_lo <= Ytest) & (Ytest <= Y_up))\n",
    "        print(f'marginal coverage of density model is {marginal_coverage}')\n",
    "\n",
    "        cover = (Y_lo <= Ytest) & (Ytest <= Y_up)\n",
    "        cc = 1\n",
    "        for eachs in set(sen):\n",
    "            cc = min(cc, cover[sen == eachs].mean())\n",
    "\n",
    "        conditional_coverage.append(cc)\n",
    "        print(f'Average conditional coverage of density model is {cc}')\n",
    "        coverage.append(np.mean((Y_lo <= Ytest) & (Ytest <= Y_up)))\n",
    "        set_size.append(np.mean(Y_up - Y_lo))\n",
    "\n",
    "        pred = np.concatenate([Y_lo.reshape(-1,1), Y_up.reshape(-1,1)], axis = 1)\n",
    "        wslab = wsc_unbiased(Xtest, Ytest, pred, delta=args.wsc_delta, M=max(200,Xtest.shape[1]+50), test_size=0.5, random_state=2020, verbose=False) \n",
    "        print(f'wsc of density model is {wslab}')\n",
    "        est_mean = np.mean(samples, axis = 1)\n",
    "\n",
    "        mdn_results = pd.concat([mdn_results, pd.DataFrame({\"iter\": [iter], \"alpha\": [alpha], \"coverage\": [np.mean((Y_lo <= Ytest) & (Ytest <= Y_up))], \\\n",
    "        \"set_size\": [np.mean(Y_up - Y_lo)], \"conditional_coverage\": [np.min(cc)], 'mse': [((Ytest - est_mean)**2).mean()], \n",
    "        'wslab': [wslab]})])\n",
    "    print('evaluate genmodel done.')\n",
    "\n",
    "    mdn_results.to_csv(f'./log/{args.dataset}_{args.densitymodel}_genmodel_coverage.csv', index=False)\n",
    "    mdn_results_mean = mdn_results.groupby(['alpha']).mean().reset_index()\n",
    "    mdn_results_se = mdn_results.groupby(['alpha']).std().reset_index() / np.sqrt(5)\n",
    "    mdn_results_se.columns = [each + '_se' for each in mdn_results_se.columns]\n",
    "    mdn_results_sum = pd.concat([mdn_results_mean, mdn_results_se], axis = 1)\n",
    "    mdn_results_sum.to_csv(f'./log/{args.dataset}_{args.densitymodel}_genmodel_summary.csv', index=False)\n",
    "\n",
    "    # evaluate basemodel \n",
    "    Ymodel_base = basemodel.fit(X, Y)\n",
    "    Yhat = Ymodel_base.predict(Xval)\n",
    "    Yscore = conformalScore(Yval, Yhat, sd = val_std)\n",
    "    Yhat_test = Ymodel_base.predict(Xtest)\n",
    "    for alpha in alpha_grid:\n",
    "        qhat = np.quantile(Yscore, np.ceil((nval+1)*(1-alpha))/nval, interpolation='higher')\n",
    "        Yslack = qhat\n",
    "\n",
    "        print(f'cutoff is {Yslack}')\n",
    "        if args.conformalscore == 'residual':\n",
    "            Ylo = Yhat_test - Yslack\n",
    "            Yup = Yhat_test + Yslack\n",
    "        elif args.conformalscore == 'normalized':\n",
    "            Ylo = Yhat_test - Yslack * test_std\n",
    "            Yup = Yhat_test + Yslack * test_std\n",
    "\n",
    "        CI = pd.DataFrame({\"lower\": Ylo.reshape(-1), \"upper\": Yup.reshape(-1)})\n",
    "\n",
    "        # check subgroup coverage\n",
    "        cover = (Ylo <= Ytest) & (Ytest <= Yup)\n",
    "        cc = 1\n",
    "        for eachs in set(sen):\n",
    "            cc = min(cc, cover[sen == eachs].mean())\n",
    "\n",
    "        conditional_coverage.append(cc)\n",
    "        print(f'Coverage is {np.mean((Ylo <= Ytest) & (Ytest <= Yup))}')\n",
    "        print(f'Average coverage set size is {np.mean(Yup - Ylo)}')\n",
    "        print(f'Average conditional coverage is {cc}')\n",
    "        coverage.append(np.mean((Ylo <= Ytest) & (Ytest <= Yup)))\n",
    "        set_size.append(np.mean(Yup - Ylo))\n",
    "\n",
    "        pred = np.concatenate([Ylo.reshape(-1,1), Yup.reshape(-1,1)], axis = 1)\n",
    "        wslab = wsc_unbiased(Xtest, Ytest, pred, delta=args.wsc_delta, M=max(200,Xtest.shape[1]+50), test_size=0.5, random_state=2020, verbose=False)\n",
    "        print(f'wsc is {wslab}')\n",
    "\n",
    "        covered_index = ((Ylo <= Ytest) & (Ytest <= Yup))\n",
    "        base_results = pd.concat([base_results, pd.DataFrame({\"iter\": [iter], \"alpha\": [alpha], \"coverage\": [np.mean((Ylo <= Ytest) & (Ytest <= Yup))], \\\n",
    "        \"set_size\": [np.mean(Yup - Ylo)], \"conditional_coverage\": [np.min(cc)], 'mse': [((Ytest - Yhat_test)**2).mean()], \n",
    "        \"wslab\": [wslab]})])\n",
    "\n",
    "        filename = f'./log/{args.dataset}_{args.model}_{args.densitymodel}_{args.conformalscore}_base_coverage.csv'\n",
    "        filename = filename.replace('_cc', '')\n",
    "        base_results.to_csv(filename, index=False)\n",
    "        base_results_mean = base_results.groupby(['alpha']).mean().reset_index()\n",
    "        base_results_se = base_results.groupby(['alpha']).std().reset_index() / np.sqrt(5)\n",
    "        base_results_se.columns = [each + '_se' for each in base_results_se.columns]\n",
    "        base_results_sum = pd.concat([base_results_mean, base_results_se], axis = 1)\n",
    "        filename = f'./log/{args.dataset}_{args.model}_{args.densitymodel}_{args.conformalscore}_base_summary.csv'\n",
    "        filename = filename.replace('_cc', '')\n",
    "        base_results_sum.to_csv(filename, index=False)\n",
    "\n",
    "if model == 'linear_cc':\n",
    "    linearmodel_cc = TorchLinearModel_CC(X.shape[1], lamb=args.lamb, density_model = density_model)\n",
    "    Ymodel = linearmodel_cc.fit(X, Y, Xval, Yval, Xtest, Ytest, sen, i = iter)\n",
    "elif model == 'mlp_cc':\n",
    "    linearmodel_cc = TorchLinearModel_CC(X.shape[1], lamb=args.lamb, density_model = density_model)\n",
    "    Ymodel = linearmodel_cc.fit(X, Y, Xval, Yval, Xtest, Ytest, sen, i = iter)\n",
    "elif model == 'linear' or model == 'mlp':\n",
    "    linearmodel = TorchBaseModel(X.shape[1])\n",
    "    Ymodel = linearmodel.fit(X, Y)\n",
    "\n",
    "Yhat = Ymodel.predict(Xval)\n",
    "Yscore = conformalScore(Yval, Yhat, sd = val_std)\n",
    "Yhat_test = Ymodel.predict(Xtest)\n",
    "\n",
    "alpha = 0.1 \n",
    "qhat = np.quantile(Yscore, np.ceil((nval+1)*(1-alpha))/nval, interpolation='higher')\n",
    "Yslack = qhat\n",
    "\n",
    "print(f'cutoff is {Yslack}')\n",
    "if args.conformalscore == 'residual':\n",
    "    Ylo = Yhat_test - Yslack\n",
    "    Yup = Yhat_test + Yslack\n",
    "elif args.conformalscore == 'normalized':\n",
    "    Ylo = Yhat_test - Yslack * test_std\n",
    "    Yup = Yhat_test + Yslack * test_std\n",
    "\n",
    "CI = pd.DataFrame({\"lower\": Ylo.reshape(-1), \"upper\": Yup.reshape(-1)})\n",
    "\n",
    "# check subgroup coverage\n",
    "cover = (Ylo <= Ytest) & (Ytest <= Yup)\n",
    "cc = 1\n",
    "for eachs in set(sen):\n",
    "    cc = min(cc, cover[sen == eachs].mean())\n",
    "\n",
    "conditional_coverage.append(cc)\n",
    "print(f'Coverage is {np.mean((Ylo <= Ytest) & (Ytest <= Yup))}')\n",
    "print(f'Average coverage set size is {np.mean(Yup - Ylo)}')\n",
    "print(f'Average conditional coverage is {cc}')\n",
    "coverage.append(np.mean((Ylo <= Ytest) & (Ytest <= Yup)))\n",
    "set_size.append(np.mean(Yup - Ylo))\n",
    "\n",
    "pred = np.concatenate([Ylo.reshape(-1,1), Yup.reshape(-1,1)], axis = 1)\n",
    "wslab = wsc_unbiased(Xtest, Ytest, pred, delta=0.1, M=max(200,Xtest.shape[1]+50), test_size=0.5, random_state=2020, verbose=False)\n",
    "print(f'wsc is {wslab}')\n",
    "\n",
    "covered_index = ((Ylo <= Ytest) & (Ytest <= Yup))\n",
    "results = pd.concat([results, pd.DataFrame({\"iter\": [iter], \"alpha\": [alpha], \"coverage\": [np.mean((Ylo <= Ytest) & (Ytest <= Yup))], \\\n",
    "\"set_size\": [np.mean(Yup - Ylo)], \"conditional_coverage\": [np.min(cc)], 'mse': [((Ytest - Yhat_test)**2).mean()], \n",
    "\"wslab\": [wslab]})])\n",
    "\n",
    "results.to_csv(f'./log/{args.dataset}_{args.model}_{args.lamb}_{args.densitymodel}_{args.conformalscore}_coverage.csv', index=False)\n",
    "\n",
    "result_mean = results.groupby(['alpha']).mean().reset_index()\n",
    "result_se = results.groupby(['alpha']).std().reset_index() / np.sqrt(5)\n",
    "result_se.columns = [each + '_se' for each in result_se.columns]\n",
    "result_sum = pd.concat([result_mean, result_se], axis = 1)\n",
    "result_sum.to_csv(f'./log/{args.dataset}_{args.model}_{args.lamb}_{args.densitymodel}_{args.conformalscore}_coverage_summary.csv', index=False)\n",
    "    \n",
    "print(f'WSLAB is {wslab}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdeenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
