{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruijiang/.conda/envs/conformalcc/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is  cuda:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os \n",
    "\n",
    "import string \n",
    "from scipy.stats import norm\n",
    "import torch \n",
    "#import torchsort \n",
    "import copy \n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.distributions as dist\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from dataset_prepare import * \n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "from probaforms.models import CVAE\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--n', type=int, default=2000)\n",
    "parser.add_argument('--d', type=int, default=1)\n",
    "parser.add_argument('--nval', type=int, default=1000)\n",
    "parser.add_argument('--alpha', type=float, default=0.1)\n",
    "parser.add_argument('--niter', type=int, default=100)\n",
    "parser.add_argument('--densitymodel', type=str, default='CVAE')\n",
    "parser.add_argument('--dataset', type=str, default='meps_19')\n",
    "parser.add_argument('--lamb', type=float, default=100)\n",
    "parser.add_argument('--model', type=str, default='linear')\n",
    "parser.add_argument('--conformalscore', type=str, default='residual')\n",
    "parser.add_argument('--wsc_delta', type=float, default=0.1)\n",
    "\n",
    "args = parser.parse_args([])\n",
    "n = args.n\n",
    "d = args.d\n",
    "alpha = args.alpha\n",
    "niter = args.niter\n",
    "model = args.model\n",
    "densitymodel = args.densitymodel\n",
    "dataset_name = args.dataset\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = 'cpu'\n",
    "print('device is ', device)\n",
    "\n",
    "def conformalScore(Y, Yhat, sd = 1):\n",
    "    if args.conformalscore == 'residual':\n",
    "        score = np.abs(Yhat - Y)\n",
    "    elif args.conformalscore == 'normalized':\n",
    "        score = np.abs(Yhat - Y) / sd\n",
    "    return score\n",
    "\n",
    "def conformalScore_torch(Y, Yhat, sd = 1):\n",
    "    if args.conformalscore == 'residual':\n",
    "        score = torch.abs(Yhat - Y)\n",
    "    elif args.conformalscore == 'normalized':\n",
    "        score = torch.abs(Yhat - Y) / sd\n",
    "    return score\n",
    "\n",
    "\n",
    "outfun = LinearRegression()\n",
    "\n",
    "# torch linear model \n",
    "class LinearModel(torch.nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(d, 1)\n",
    "        torch.nn.init.xavier_uniform_(self.linear.weight)\n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "    \n",
    "\n",
    "# a MLP with LeakyReLU activation\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super(MLP, self).__init__()\n",
    "        hidd = 16\n",
    "        self.linear1 = torch.nn.Linear(d, hidd)\n",
    "        self.linear2 = torch.nn.Linear(hidd, hidd)\n",
    "        self.linear3 = torch.nn.Linear(hidd, 1)\n",
    "        self.leakyrelu = torch.nn.LeakyReLU()\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        # initialize the weights\n",
    "        torch.nn.init.xavier_uniform_(self.linear1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.linear2.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.linear3.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear1(x)\n",
    "        y_pred = self.leakyrelu(y_pred)\n",
    "        y_pred = self.linear2(y_pred)\n",
    "        y_pred = self.leakyrelu(y_pred)\n",
    "        y_pred = self.linear3(y_pred)\n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "# a class for the torch linear model with fit and predict methods\n",
    "class TorchBaseModel():\n",
    "    def __init__(self, d):\n",
    "        if args.model == 'linear' or args.model == 'linear_cc':\n",
    "            self.model = LinearModel(d).to(device)\n",
    "        elif args.model == 'mlp' or args.model == 'mlp_cc':\n",
    "            self.model = MLP(d).to(device)\n",
    "        self.criterion = torch.nn.MSELoss(reduction='mean')\n",
    "        #self.optimizer = torch.optim.SGD(self.model.parameters(), lr=1e-3)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        X = torch.from_numpy(X).float().to(device)\n",
    "        Y = torch.from_numpy(Y).float().to(device)\n",
    "        for t in range(10000):\n",
    "            # Forward pass: Compute predicted y by passing x to the model\n",
    "            y_pred = self.model(X)\n",
    "            # Compute and print loss\n",
    "            loss = ((y_pred.reshape(-1) - Y)**2).mean()\n",
    "            # Zero gradients, perform a backward pass, and update the weights.\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            if t % 100 == 99:\n",
    "                print(t, loss.item())\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = torch.from_numpy(X).float().to(device)\n",
    "        return self.model(X).detach().cpu().numpy().reshape(-1)\n",
    "\n",
    "class TorchLinearModel_CC():\n",
    "    def __init__(self, d, lamb = 1, density_model = None):\n",
    "        if args.model == 'linear_cc':\n",
    "            self.model = LinearModel(d).to(device)\n",
    "        elif args.model == 'mlp_cc':\n",
    "            self.model = MLP(d).to(device)\n",
    "        self.lamb = lamb\n",
    "        self.criterion = torch.nn.MSELoss(reduction='mean')\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        self.density_model = density_model\n",
    "        \n",
    "    def fit(self, X, Y, X_CC, Y_CC, Xtest, Ytest, sen, i = 0):\n",
    "        # plot the data\n",
    "        X = torch.from_numpy(X).float().to(device)\n",
    "        Y = torch.from_numpy(Y).float().to(device)\n",
    "        Xval = X_CC\n",
    "        Yval = Y_CC\n",
    "        X_CC = torch.from_numpy(X_CC).float().to(device)\n",
    "        Y_CC = torch.from_numpy(Y_CC).float().to(device)\n",
    "        X_CC = X\n",
    "        Y_CC = Y\n",
    "        \n",
    "        #first train a simple linear model\n",
    "        for t in range(10000):\n",
    "            # Forward pass: Compute predicted y by passing x to the model\n",
    "            y_pred = self.model(X)\n",
    "            # Compute and print loss\n",
    "            loss = ((y_pred.reshape(-1) - Y)**2).mean()\n",
    "            \n",
    "            # Zero gradients, perform a backward pass, and update the weights.\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        nsamples = 2000\n",
    "        samples = np.zeros((X_CC.shape[0], nsamples))\n",
    "        for i in range(nsamples):\n",
    "            condX = X_CC.cpu().numpy()\n",
    "            samples[:,i] = self.density_model.sample(condX).reshape(-1)\n",
    "        sd_cc = np.std(samples, axis = 1)\n",
    "        sd_cc = torch.from_numpy(sd_cc).float().to(device)\n",
    "\n",
    "        samples_test = np.zeros((Xtest.shape[0], nsamples))\n",
    "        for i in range(nsamples):\n",
    "            condX = Xtest\n",
    "            samples_test[:,i] = self.density_model.sample(condX).reshape(-1)\n",
    "        sd_test = np.std(samples_test, axis = 1)\n",
    "        sd_test = torch.from_numpy(sd_test).float().to(device)\n",
    "\n",
    "        best_loss = 1e7\n",
    "        improved_in_last = 0\n",
    "        for t in range(60):\n",
    "            # train a density model \n",
    "            y_pred = self.model(X_CC)\n",
    "            V_CC = conformalScore_torch(Y_CC, y_pred.reshape(-1), sd = sd_cc)\n",
    "            \n",
    "            y_pred = self.model(X)\n",
    "            loss = ((y_pred.reshape(-1) - Y)**2).mean()\n",
    "            \n",
    "            y_pred = self.model(X_CC)\n",
    "            V_CC = conformalScore_torch(Y_CC, y_pred.reshape(-1), sd = sd_cc)\n",
    "            \n",
    "            wdiv = 0 \n",
    "            nsamples = 500\n",
    "            nsamples = min(nsamples, X_CC.shape[0])\n",
    "            batch_size = 100\n",
    "            temperature = 10\n",
    "            # random sample nsamples from X_CC\n",
    "            rand_index = np.random.choice(X_CC.shape[0], nsamples)\n",
    "            wdiv = torch.zeros_like(V_CC[:nsamples])\n",
    "            for indi, i in enumerate(rand_index):\n",
    "                if densitymodel == 'mdn':\n",
    "                    thissampled = torch.Tensor(self.density_model.sample((torch.ones_like(X_CC) * X_CC[i,])[:nsamples].cpu().numpy())[1].reshape(-1))\n",
    "                else:\n",
    "                    condX = (torch.ones_like(X_CC) * X_CC[i,])[:nsamples].cpu().numpy()\n",
    "                    #thissampled = self.density_model.generate(condX.shape[0], cond = condX).values.reshape(-1)\n",
    "                    thissampled = self.density_model.sample(condX).reshape(-1)\n",
    "                    thissampled = torch.Tensor(thissampled)\n",
    "                thissampled = thissampled.to(device)\n",
    "                sample_index = np.random.choice(X_CC.shape[0], nsamples)\n",
    "                VCC_sampled = V_CC[sample_index]\n",
    "                V_givenx = conformalScore_torch(thissampled, torch.ones_like(y_pred.reshape(-1)[:nsamples]) * y_pred[i,], \\\n",
    "                                sd = torch.ones_like(y_pred.reshape(-1)[:nsamples]) * sd_cc[i])\n",
    "\n",
    "                diff = approx_ecdf(V_givenx, VCC_sampled, grid_size = 100, temperature = 10)\n",
    "                # when the max cannot be reduced effectively, minimize the softmax is still effective for improving the cc\n",
    "                wdiv[indi] = (diff * F.softmax(diff * temperature, dim = 0)).sum()\n",
    "\n",
    "            div = wdiv * F.softmax(wdiv * temperature, dim = 0)\n",
    "            div = div.sum()\n",
    "            loss1 = loss + self.lamb * div.mean()\n",
    "            print('t = ', t, 'loss = ', loss, 'div = ', div, 'loss1 = ', loss1)\n",
    "\n",
    "            # Zero gradients, perform a backward pass, and update the weights.\n",
    "            self.optimizer.zero_grad()\n",
    "            loss1.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if loss1 < best_loss:\n",
    "                best_loss = loss1\n",
    "                best_model = copy.deepcopy(self.model)\n",
    "                improved_in_last = 0\n",
    "            \n",
    "            if (t+1) % 20 == 0:\n",
    "                alpha = 0.1 \n",
    "                Yhat = self.predict(Xval)\n",
    "                Yscore = conformalScore(Yval, Yhat, sd = sd_cc.cpu().numpy())\n",
    "\n",
    "                Yhat_test = self.predict(Xtest)\n",
    "                \n",
    "                nval = Xval.shape[0]\n",
    "                qhat = np.quantile(Yscore, np.ceil((nval+1)*(1-alpha))/nval, interpolation='higher')\n",
    "                Yslack = qhat\n",
    "\n",
    "                print(f'cutoff is {Yslack}')\n",
    "                if args.conformalscore == 'residual':\n",
    "                    Ylo = Yhat_test - Yslack\n",
    "                    Yup = Yhat_test + Yslack\n",
    "                elif args.conformalscore == 'normalized':\n",
    "                    Ylo = Yhat_test - Yslack * sd_test.cpu().numpy()\n",
    "                    Yup = Yhat_test + Yslack * sd_test.cpu().numpy()\n",
    "\n",
    "                CI = pd.DataFrame({\"lower\": Ylo.reshape(-1), \"upper\": Yup.reshape(-1)})\n",
    "\n",
    "                cover = (Ylo <= Ytest) & (Ytest <= Yup)\n",
    "                cc = 100\n",
    "                for eachs in set(sen):\n",
    "                    cc = min(cc, cover[sen == eachs].mean())\n",
    "\n",
    "                pred = np.concatenate([Ylo.reshape(-1,1), Yup.reshape(-1,1)], axis = 1)\n",
    "                wslab = wsc_unbiased(Xtest, Ytest, pred, delta=args.wsc_delta, M=max(200,Xtest.shape[1]+50), test_size=0.5, random_state=2020, verbose=False)\n",
    "                print(t, loss.item())\n",
    "                print(f'mc is {np.mean((Ylo <= Ytest) & (Ytest <= Yup))}')\n",
    "                print(f'cc is {cc}')\n",
    "                print(f'wsc is {wslab}')\n",
    "        \n",
    "        y_pred = self.model(X_CC).reshape(-1)\n",
    "        V_CC = conformalScore_torch(Y_CC, y_pred)\n",
    "        # plot V_CC\n",
    "        self.model = best_model\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = torch.from_numpy(X).float().to(device)\n",
    "        return self.model(X).detach().cpu().numpy().reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random \n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "def wsc(X, y, pred, delta=0.1, M=1000, verbose=False):\n",
    "    # Extract lower and upper prediction bands\n",
    "    pred_l = np.min(pred,1)\n",
    "    pred_h = np.max(pred,1)\n",
    "\n",
    "    def wsc_v(X, y, pred_l, pred_h, delta, v):\n",
    "        n = len(y)\n",
    "        cover = (y>=pred_l)*(y<=pred_h)\n",
    "        z = np.dot(X,v)\n",
    "        # Compute mass\n",
    "        z_order = np.argsort(z)\n",
    "        z_sorted = z[z_order]\n",
    "        cover_ordered = cover[z_order]\n",
    "        ai_max = int(np.round((1.0-delta)*n))\n",
    "        ai_best = 0\n",
    "        bi_best = n-1\n",
    "        cover_min = np.mean((y >= pred_l)*(y <= pred_h))\n",
    "        for ai in np.arange(0, ai_max):\n",
    "            bi_min = np.minimum(ai+int(np.round(delta*n)),n)\n",
    "            coverage = np.cumsum(cover_ordered[ai:n]) / np.arange(1,n-ai+1)\n",
    "            coverage[np.arange(0,bi_min-ai)]=1\n",
    "            bi_star = ai+np.argmin(coverage)\n",
    "            cover_star = coverage[bi_star-ai]\n",
    "            if cover_star < cover_min:\n",
    "                ai_best = ai\n",
    "                bi_best = bi_star\n",
    "                cover_min = cover_star\n",
    "        return cover_min, z_sorted[ai_best], z_sorted[bi_best]\n",
    "\n",
    "    def sample_sphere(n, p):\n",
    "        v = np.random.randn(p, n)\n",
    "        v /= np.linalg.norm(v, axis=0)\n",
    "        v = v.T\n",
    "        return v\n",
    "\n",
    "    seed_everything(2020)\n",
    "    V = sample_sphere(M, p=X.shape[1])\n",
    "    wsc_list = [[]] * M\n",
    "    a_list = [[]] * M\n",
    "    b_list = [[]] * M\n",
    "    if verbose:\n",
    "        for m in tqdm(range(M)):\n",
    "            wsc_list[m], a_list[m], b_list[m] = wsc_v(X, y, pred_l, pred_h, delta, V[m])\n",
    "    else:\n",
    "        for m in range(M):\n",
    "            wsc_list[m], a_list[m], b_list[m] = wsc_v(X, y, pred_l, pred_h, delta, V[m])\n",
    "        \n",
    "    idx_star = np.argmin(np.array(wsc_list))\n",
    "    a_star = a_list[idx_star]\n",
    "    b_star = b_list[idx_star]\n",
    "    v_star = V[idx_star]\n",
    "    wsc_star = wsc_list[idx_star]\n",
    "    return wsc_star, v_star, a_star, b_star\n",
    "\n",
    "def wsc_unbiased(X, y, pred, delta=0.1, M=1000, test_size=0.75, random_state=2020, verbose=False):\n",
    "    test_size = 0.75\n",
    "    M = 1000\n",
    "    def wsc_vab(X, y, pred, v, a, b):\n",
    "        # Extract lower and upper prediction bands\n",
    "        pred_l = np.min(pred,1)\n",
    "        pred_h = np.max(pred,1)\n",
    "        n = len(y)\n",
    "        cover = (y>=pred_l)*(y<=pred_h)\n",
    "        z = np.dot(X,v)\n",
    "        idx = np.where((z>=a)*(z<=b))\n",
    "        coverage = np.mean(cover[idx])\n",
    "        return coverage\n",
    "\n",
    "    seed_everything(random_state)\n",
    "    X_train, X_test, y_train, y_test, pred_train, pred_test = train_test_split(X, y, pred, test_size=test_size,\n",
    "                                                                         random_state=random_state)\n",
    "    # Find adversarial parameters\n",
    "    wsc_star, v_star, a_star, b_star = wsc(X_train, y_train, pred_train, delta=delta, M=M, verbose=verbose)\n",
    "    # Estimate coverage\n",
    "    coverage = wsc_vab(X_test, y_test, pred_test, v_star, a_star, b_star)\n",
    "    return coverage\n",
    "\n",
    "\n",
    "import random \n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "def approx_ecdf(points1, points2, grid_size = 100, temperature = 5):\n",
    "    tgrid = torch.linspace(min(min(points1), min(points2)).item()-0.1, max(max(points1),max(points2)).item()+0.1, grid_size)\n",
    "    tgrid = tgrid.to(device)\n",
    "    cdf1 = (1-torch.sigmoid(temperature * (points1.reshape(-1,1) - tgrid))).mean(0)\n",
    "    cdf2 = (1-torch.sigmoid(temperature * (points2.reshape(-1,1) - tgrid))).mean(0)\n",
    "    diff = torch.abs(cdf1 - cdf2)\n",
    "    return diff \n",
    "\n",
    "coverage = []\n",
    "conditional_coverage = []\n",
    "set_size = []\n",
    "alpha_grid = np.linspace(0.1, 0.9, 10)\n",
    "results = pd.DataFrame({\"iter\": [], \"alpha\": [], \"coverage\": [], \"set_size\": [], \"conditional_coverage\": [], 'mse': [], 'wslab': []})\n",
    "mdn_results = pd.DataFrame({\"iter\": [], \"alpha\": [], \"coverage\": [], \"set_size\": [], \"conditional_coverage\": [], 'mse': [], 'wslab': []})\n",
    "base_results = pd.DataFrame({\"iter\": [], \"alpha\": [], \"coverage\": [], \"set_size\": [], \"conditional_coverage\": [], 'mse': [], 'wslab': []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size is 10428, number of features is 139.\n",
      "train size is 6256, val size is 2085, test size is 2087.\n",
      "density model fitting done.\n",
      "99 0.8078312873840332\n",
      "199 0.6219979524612427\n",
      "299 0.590796172618866\n",
      "399 0.5858684182167053\n",
      "499 0.5849030017852783\n",
      "599 0.5845747590065002\n",
      "699 0.5844173431396484\n",
      "799 0.5843259692192078\n",
      "899 0.5842612385749817\n",
      "999 0.5842065215110779\n",
      "1099 0.5841558575630188\n",
      "1199 0.5841073393821716\n",
      "1299 0.5840607285499573\n",
      "1399 0.5840161442756653\n",
      "1499 0.5839739441871643\n",
      "1599 0.5839343667030334\n",
      "1699 0.5838974714279175\n",
      "1799 0.5838636159896851\n",
      "1899 0.5838327407836914\n",
      "1999 0.5838049650192261\n",
      "2099 0.5837801098823547\n",
      "2199 0.5837582349777222\n",
      "2299 0.5837391018867493\n",
      "2399 0.5837225317955017\n",
      "2499 0.5837084650993347\n",
      "2599 0.5836965441703796\n",
      "2699 0.5836866497993469\n",
      "2799 0.5836784839630127\n",
      "2899 0.5836718678474426\n",
      "2999 0.5836665630340576\n",
      "3099 0.5836623311042786\n",
      "3199 0.5836589932441711\n",
      "3299 0.5836564898490906\n",
      "3399 0.583654522895813\n",
      "3499 0.5836530923843384\n",
      "3599 0.5836520195007324\n",
      "3699 0.5836512446403503\n",
      "3799 0.5836506485939026\n",
      "3899 0.5836502313613892\n",
      "3999 0.5836499333381653\n",
      "4099 0.583649754524231\n",
      "4199 0.5836498141288757\n",
      "4299 0.5836494565010071\n",
      "4399 0.5836544632911682\n",
      "4499 0.5836493372917175\n",
      "4599 0.5836492776870728\n",
      "4699 0.5836494565010071\n",
      "4799 0.5836492776870728\n",
      "4899 0.583649218082428\n",
      "4999 0.5836582779884338\n",
      "5099 0.583649218082428\n",
      "5199 0.5836491584777832\n",
      "5299 0.5836514830589294\n",
      "5399 0.5836491584777832\n",
      "5499 0.583649218082428\n",
      "5599 0.5836491584777832\n",
      "5699 0.5836491584777832\n",
      "5799 0.583656370639801\n",
      "5899 0.5836491584777832\n",
      "5999 0.5836491584777832\n",
      "6099 0.583649218082428\n",
      "6199 0.5836491584777832\n",
      "6299 0.5836512446403503\n",
      "6399 0.5836491584777832\n",
      "6499 0.5836634635925293\n",
      "6599 0.5836490988731384\n",
      "6699 0.5836493968963623\n",
      "6799 0.5836490988731384\n",
      "6899 0.5836491584777832\n",
      "6999 0.5836491584777832\n",
      "7099 0.5836490988731384\n",
      "7199 0.5836507081985474\n",
      "7299 0.5836491584777832\n",
      "7399 0.5836491584777832\n",
      "7499 0.5836491584777832\n",
      "7599 0.5836490392684937\n",
      "7699 0.5836569666862488\n",
      "7799 0.5836490988731384\n",
      "7899 0.5836490988731384\n",
      "7999 0.5836495757102966\n",
      "8099 0.5836490988731384\n",
      "8199 0.5836490988731384\n",
      "8299 0.5836491584777832\n",
      "8399 0.5836490988731384\n",
      "8499 0.5836647748947144\n",
      "8599 0.5836490392684937\n",
      "8699 0.5836490988731384\n",
      "8799 0.5836493968963623\n",
      "8899 0.5836490392684937\n",
      "8999 0.5836491584777832\n",
      "9099 0.5836491584777832\n",
      "9199 0.5836490988731384\n",
      "9299 0.583652138710022\n",
      "9399 0.5836490988731384\n",
      "9499 0.5836490988731384\n",
      "9599 0.5836495161056519\n",
      "9699 0.5836490988731384\n",
      "9799 0.5836490392684937\n",
      "9899 0.5836491584777832\n",
      "9999 0.5836490988731384\n",
      "cutoff is 1.2113856077194214\n",
      "Coverage is 0.906085289889794\n",
      "Average coverage set size is 2.4227712154388428\n",
      "Average conditional coverage is 0.8695652173913043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_104074/2293351234.py:177: DeprecationWarning: the `interpolation=` argument to quantile was renamed to `method=`, which has additional options.\n",
      "Users of the modes 'nearest', 'lower', 'higher', or 'midpoint' are encouraged to review the method they used. (Deprecated NumPy 1.22)\n",
      "  qhat = np.quantile(Yscore, np.ceil((nval+1)*(1-alpha))/nval, interpolation='higher')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wsc is 0.7313432835820896\n",
      "WSLAB is 0.7313432835820896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_104074/2293351234.py:214: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  result_mean = results.groupby(['alpha']).mean().reset_index()\n",
      "/tmp/ipykernel_104074/2293351234.py:215: FutureWarning: The default value of numeric_only in DataFrameGroupBy.std is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  result_se = results.groupby(['alpha']).std().reset_index() / np.sqrt(5)\n"
     ]
    }
   ],
   "source": [
    "# mlp model \n",
    "seed_everything(0)\n",
    "X, Y = GetDataset(dataset_name, './data/')\n",
    "Y += 1e-3 * np.random.normal(size=Y.shape)\n",
    "\n",
    "basemodel = TorchBaseModel(X.shape[1])\n",
    "\n",
    "index = np.random.permutation(X.shape[0])\n",
    "print(f'dataset size is {X.shape[0]}, number of features is {X.shape[1]}.')\n",
    "nval = int(X.shape[0] * 0.2)\n",
    "ntrain = int(X.shape[0] * 0.6)\n",
    "Xval = X[index[:nval],:]\n",
    "Yval = Y[index[:nval]]\n",
    "Xtest = X[index[(nval+ntrain):],:]\n",
    "Ytest = Y[index[(nval+ntrain):]]\n",
    "X = X[index[(nval):(nval+ntrain)],:]\n",
    "Y = Y[index[(nval):(nval+ntrain)]]\n",
    "print(f'train size is {X.shape[0]}, val size is {Xval.shape[0]}, test size is {Xtest.shape[0]}.')\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "\n",
    "y_scaler = StandardScaler()\n",
    "y_scaler.fit(Y.reshape(-1,1))\n",
    "\n",
    "Y = y_scaler.transform(Y.reshape(-1,1)).reshape(-1)\n",
    "Yval = y_scaler.transform(Yval.reshape(-1,1)).reshape(-1)\n",
    "Ytest = y_scaler.transform(Ytest.reshape(-1,1)).reshape(-1)\n",
    "\n",
    "if dataset_name == 'meps_19' or dataset_name == 'meps_20' or dataset_name == 'meps_21':\n",
    "    # define a subgroup - checking some surrogate variable\n",
    "    sen = np.zeros_like(Ytest)\n",
    "    sen[(Xtest[:, 9] == 1) & (Xtest[:, -1] == 1)] = 0\n",
    "    sen[(Xtest[:, 9] == 1) & (Xtest[:, -1] == 0)] = 1\n",
    "    sen[(Xtest[:, 9] == 0) & (Xtest[:, -1] == 1)] = 2\n",
    "    sen[(Xtest[:, 9] == 0) & (Xtest[:, -1] == 0)] = 3\n",
    "    \n",
    "X = scaler.transform(X)\n",
    "Xval = scaler.transform(Xval)\n",
    "Xtest = scaler.transform(Xtest)\n",
    "\n",
    "if (model == 'linear' or model == 'mlp') and args.conformalscore == 'residual':\n",
    "    pass \n",
    "elif args.densitymodel == 'cvae':\n",
    "    density_model = CVAE(n_epochs=500, latent_dim=8, hidden = (32,32), batch_size=128, lr=1e-3, weight_decay=1e-4, verbose = 1)\n",
    "    density_model.fit(Y.reshape(-1,1), X)\n",
    "\n",
    "print('density model fitting done.')\n",
    "nval = Xval.shape[0]\n",
    "\n",
    "nsamples = 1000\n",
    "\n",
    "if (model == 'linear' or model == 'mlp') and args.conformalscore == 'residual':\n",
    "    val_std = 1\n",
    "    test_std = 1\n",
    "else:\n",
    "    samples_val = np.zeros((nval, nsamples))\n",
    "    for i in range(nsamples):\n",
    "        #samples_val[:,i] = density_model.sample(Xval)[1].reshape(-1)\n",
    "        #samples_val[:,i] = density_model.generate(Xval.shape[0], cond = Xval).values.reshape(-1)\n",
    "        samples_val[:,i] = density_model.sample(Xval).reshape(-1)\n",
    "\n",
    "    val_std = np.std(samples_val, axis = 1)\n",
    "\n",
    "    ntest = Xtest.shape[0]\n",
    "    samples = np.zeros((ntest, nsamples))\n",
    "    for i in range(nsamples):\n",
    "        #samples[:,i] = density_model.sample(Xtest)[1].reshape(-1)\n",
    "        #samples[:,i] = density_model.generate(Xtest.shape[0], cond = Xtest).values.reshape(-1)\n",
    "        samples[:,i] = density_model.sample(Xtest).reshape(-1)\n",
    "\n",
    "    test_std = np.std(samples, axis = 1)\n",
    "\n",
    "    # evaluate genmodel \n",
    "    print('evaluate genmodel')\n",
    "    for alpha in alpha_grid:\n",
    "        # evaluate density model's conditional/marginal coverage\n",
    "        Y_lo = np.quantile(samples, alpha/2, axis = 1)\n",
    "        Y_up = np.quantile(samples, 1 - alpha/2, axis = 1)\n",
    "\n",
    "        marginal_coverage = np.mean((Y_lo <= Ytest) & (Ytest <= Y_up))\n",
    "        print(f'marginal coverage of density model is {marginal_coverage}')\n",
    "\n",
    "        cover = (Y_lo <= Ytest) & (Ytest <= Y_up)\n",
    "        cc = 1\n",
    "        for eachs in set(sen):\n",
    "            cc = min(cc, cover[sen == eachs].mean())\n",
    "\n",
    "        conditional_coverage.append(cc)\n",
    "        print(f'Average conditional coverage of density model is {cc}')\n",
    "        coverage.append(np.mean((Y_lo <= Ytest) & (Ytest <= Y_up)))\n",
    "        set_size.append(np.mean(Y_up - Y_lo))\n",
    "\n",
    "        pred = np.concatenate([Y_lo.reshape(-1,1), Y_up.reshape(-1,1)], axis = 1)\n",
    "        wslab = wsc_unbiased(Xtest, Ytest, pred, delta=args.wsc_delta, M=max(200,Xtest.shape[1]+50), test_size=0.5, random_state=2020, verbose=False) \n",
    "        print(f'wsc of density model is {wslab}')\n",
    "        est_mean = np.mean(samples, axis = 1)\n",
    "\n",
    "        mdn_results = pd.concat([mdn_results, pd.DataFrame({\"iter\": [iter], \"alpha\": [alpha], \"coverage\": [np.mean((Y_lo <= Ytest) & (Ytest <= Y_up))], \\\n",
    "        \"set_size\": [np.mean(Y_up - Y_lo)], \"conditional_coverage\": [np.min(cc)], 'mse': [((Ytest - est_mean)**2).mean()], \n",
    "        'wslab': [wslab]})])\n",
    "    print('evaluate genmodel done.')\n",
    "\n",
    "    mdn_results.to_csv(f'./log/{args.dataset}_{args.densitymodel}_genmodel_coverage.csv', index=False)\n",
    "    mdn_results_mean = mdn_results.groupby(['alpha']).mean().reset_index()\n",
    "    mdn_results_se = mdn_results.groupby(['alpha']).std().reset_index() / np.sqrt(5)\n",
    "    mdn_results_se.columns = [each + '_se' for each in mdn_results_se.columns]\n",
    "    mdn_results_sum = pd.concat([mdn_results_mean, mdn_results_se], axis = 1)\n",
    "    mdn_results_sum.to_csv(f'./log/{args.dataset}_{args.densitymodel}_genmodel_summary.csv', index=False)\n",
    "\n",
    "    # evaluate basemodel \n",
    "    Ymodel_base = basemodel.fit(X, Y)\n",
    "    Yhat = Ymodel_base.predict(Xval)\n",
    "    Yscore = conformalScore(Yval, Yhat, sd = val_std)\n",
    "    Yhat_test = Ymodel_base.predict(Xtest)\n",
    "    for alpha in alpha_grid:\n",
    "        qhat = np.quantile(Yscore, np.ceil((nval+1)*(1-alpha))/nval, interpolation='higher')\n",
    "        Yslack = qhat\n",
    "\n",
    "        print(f'cutoff is {Yslack}')\n",
    "        if args.conformalscore == 'residual':\n",
    "            Ylo = Yhat_test - Yslack\n",
    "            Yup = Yhat_test + Yslack\n",
    "        elif args.conformalscore == 'normalized':\n",
    "            Ylo = Yhat_test - Yslack * test_std\n",
    "            Yup = Yhat_test + Yslack * test_std\n",
    "\n",
    "        CI = pd.DataFrame({\"lower\": Ylo.reshape(-1), \"upper\": Yup.reshape(-1)})\n",
    "\n",
    "        # check subgroup coverage\n",
    "        cover = (Ylo <= Ytest) & (Ytest <= Yup)\n",
    "        cc = 1\n",
    "        for eachs in set(sen):\n",
    "            cc = min(cc, cover[sen == eachs].mean())\n",
    "\n",
    "        conditional_coverage.append(cc)\n",
    "        print(f'Coverage is {np.mean((Ylo <= Ytest) & (Ytest <= Yup))}')\n",
    "        print(f'Average coverage set size is {np.mean(Yup - Ylo)}')\n",
    "        print(f'Average conditional coverage is {cc}')\n",
    "        coverage.append(np.mean((Ylo <= Ytest) & (Ytest <= Yup)))\n",
    "        set_size.append(np.mean(Yup - Ylo))\n",
    "\n",
    "        pred = np.concatenate([Ylo.reshape(-1,1), Yup.reshape(-1,1)], axis = 1)\n",
    "        wslab = wsc_unbiased(Xtest, Ytest, pred, delta=args.wsc_delta, M=max(200,Xtest.shape[1]+50), test_size=0.5, random_state=2020, verbose=False)\n",
    "        print(f'wsc is {wslab}')\n",
    "\n",
    "        covered_index = ((Ylo <= Ytest) & (Ytest <= Yup))\n",
    "        base_results = pd.concat([base_results, pd.DataFrame({\"iter\": [iter], \"alpha\": [alpha], \"coverage\": [np.mean((Ylo <= Ytest) & (Ytest <= Yup))], \\\n",
    "        \"set_size\": [np.mean(Yup - Ylo)], \"conditional_coverage\": [np.min(cc)], 'mse': [((Ytest - Yhat_test)**2).mean()], \n",
    "        \"wslab\": [wslab]})])\n",
    "\n",
    "        filename = f'./log/{args.dataset}_{args.model}_{args.densitymodel}_{args.conformalscore}_base_coverage.csv'\n",
    "        filename = filename.replace('_cc', '')\n",
    "        base_results.to_csv(filename, index=False)\n",
    "        base_results_mean = base_results.groupby(['alpha']).mean().reset_index()\n",
    "        base_results_se = base_results.groupby(['alpha']).std().reset_index() / np.sqrt(5)\n",
    "        base_results_se.columns = [each + '_se' for each in base_results_se.columns]\n",
    "        base_results_sum = pd.concat([base_results_mean, base_results_se], axis = 1)\n",
    "        filename = f'./log/{args.dataset}_{args.model}_{args.densitymodel}_{args.conformalscore}_base_summary.csv'\n",
    "        filename = filename.replace('_cc', '')\n",
    "        base_results_sum.to_csv(filename, index=False)\n",
    "\n",
    "if model == 'linear_cc':\n",
    "    linearmodel_cc = TorchLinearModel_CC(X.shape[1], lamb=args.lamb, density_model = density_model)\n",
    "    Ymodel = linearmodel_cc.fit(X, Y, Xval, Yval, Xtest, Ytest, sen, i = iter)\n",
    "elif model == 'mlp_cc':\n",
    "    linearmodel_cc = TorchLinearModel_CC(X.shape[1], lamb=args.lamb, density_model = density_model)\n",
    "    Ymodel = linearmodel_cc.fit(X, Y, Xval, Yval, Xtest, Ytest, sen, i = iter)\n",
    "elif model == 'linear' or model == 'mlp':\n",
    "    linearmodel = TorchBaseModel(X.shape[1])\n",
    "    Ymodel = linearmodel.fit(X, Y)\n",
    "\n",
    "Yhat = Ymodel.predict(Xval)\n",
    "Yscore = conformalScore(Yval, Yhat, sd = val_std)\n",
    "Yhat_test = Ymodel.predict(Xtest)\n",
    "\n",
    "alpha = 0.1 \n",
    "qhat = np.quantile(Yscore, np.ceil((nval+1)*(1-alpha))/nval, interpolation='higher')\n",
    "Yslack = qhat\n",
    "\n",
    "print(f'cutoff is {Yslack}')\n",
    "if args.conformalscore == 'residual':\n",
    "    Ylo = Yhat_test - Yslack\n",
    "    Yup = Yhat_test + Yslack\n",
    "elif args.conformalscore == 'normalized':\n",
    "    Ylo = Yhat_test - Yslack * test_std\n",
    "    Yup = Yhat_test + Yslack * test_std\n",
    "\n",
    "CI = pd.DataFrame({\"lower\": Ylo.reshape(-1), \"upper\": Yup.reshape(-1)})\n",
    "\n",
    "# check subgroup coverage\n",
    "cover = (Ylo <= Ytest) & (Ytest <= Yup)\n",
    "cc = 1\n",
    "for eachs in set(sen):\n",
    "    cc = min(cc, cover[sen == eachs].mean())\n",
    "\n",
    "conditional_coverage.append(cc)\n",
    "print(f'Coverage is {np.mean((Ylo <= Ytest) & (Ytest <= Yup))}')\n",
    "print(f'Average coverage set size is {np.mean(Yup - Ylo)}')\n",
    "print(f'Average conditional coverage is {cc}')\n",
    "coverage.append(np.mean((Ylo <= Ytest) & (Ytest <= Yup)))\n",
    "set_size.append(np.mean(Yup - Ylo))\n",
    "\n",
    "pred = np.concatenate([Ylo.reshape(-1,1), Yup.reshape(-1,1)], axis = 1)\n",
    "wslab = wsc_unbiased(Xtest, Ytest, pred, delta=0.1, M=max(200,Xtest.shape[1]+50), test_size=0.5, random_state=2020, verbose=False)\n",
    "print(f'wsc is {wslab}')\n",
    "\n",
    "covered_index = ((Ylo <= Ytest) & (Ytest <= Yup))\n",
    "results = pd.concat([results, pd.DataFrame({\"iter\": [iter], \"alpha\": [alpha], \"coverage\": [np.mean((Ylo <= Ytest) & (Ytest <= Yup))], \\\n",
    "\"set_size\": [np.mean(Yup - Ylo)], \"conditional_coverage\": [np.min(cc)], 'mse': [((Ytest - Yhat_test)**2).mean()], \n",
    "\"wslab\": [wslab]})])\n",
    "\n",
    "results.to_csv(f'./log/{args.dataset}_{args.model}_{args.lamb}_{args.densitymodel}_{args.conformalscore}_coverage.csv', index=False)\n",
    "\n",
    "result_mean = results.groupby(['alpha']).mean().reset_index()\n",
    "result_se = results.groupby(['alpha']).std().reset_index() / np.sqrt(5)\n",
    "result_se.columns = [each + '_se' for each in result_se.columns]\n",
    "result_sum = pd.concat([result_mean, result_se], axis = 1)\n",
    "result_sum.to_csv(f'./log/{args.dataset}_{args.model}_{args.lamb}_{args.densitymodel}_{args.conformalscore}_coverage_summary.csv', index=False)\n",
    "    \n",
    "print(f'WSLAB is {wslab}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WSLAB is 0.7313432835820896\n"
     ]
    }
   ],
   "source": [
    "print(f'WSLAB is {wslab}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size is 10428, number of features is 139.\n",
      "train size is 6256, val size is 2085, test size is 2087.\n",
      "epoch: 0, loss: 35.2357\n",
      "epoch: 10, loss: 1.3820\n",
      "epoch: 20, loss: 0.7988\n",
      "epoch: 30, loss: 0.5346\n",
      "epoch: 40, loss: 0.5216\n",
      "epoch: 50, loss: 0.4538\n",
      "epoch: 60, loss: 0.4164\n",
      "epoch: 70, loss: 0.4289\n",
      "epoch: 80, loss: 0.4257\n",
      "epoch: 90, loss: 0.4030\n",
      "epoch: 100, loss: 0.3966\n",
      "epoch: 110, loss: 0.3960\n",
      "epoch: 120, loss: 0.3836\n",
      "epoch: 130, loss: 0.3299\n",
      "epoch: 140, loss: 0.2795\n",
      "epoch: 150, loss: 0.2785\n",
      "epoch: 160, loss: 0.2731\n",
      "epoch: 170, loss: 0.2753\n",
      "epoch: 180, loss: 0.2751\n",
      "epoch: 190, loss: 0.2585\n",
      "epoch: 200, loss: 0.2606\n",
      "epoch: 210, loss: 0.2994\n",
      "epoch: 220, loss: 0.3157\n",
      "epoch: 230, loss: 0.2671\n",
      "epoch: 240, loss: 0.2478\n",
      "epoch: 250, loss: 0.2553\n",
      "epoch: 260, loss: 0.2983\n",
      "epoch: 270, loss: 0.2686\n",
      "epoch: 280, loss: 0.2472\n",
      "epoch: 290, loss: 0.2703\n",
      "epoch: 300, loss: 0.2586\n",
      "epoch: 310, loss: 0.2630\n",
      "epoch: 320, loss: 0.2637\n",
      "epoch: 330, loss: 0.2454\n",
      "epoch: 340, loss: 0.2578\n",
      "epoch: 350, loss: 0.2458\n",
      "epoch: 360, loss: 0.2608\n",
      "epoch: 370, loss: 0.2568\n",
      "epoch: 380, loss: 0.2740\n",
      "epoch: 390, loss: 0.2646\n",
      "epoch: 400, loss: 0.2474\n",
      "epoch: 410, loss: 0.2678\n",
      "epoch: 420, loss: 0.2573\n",
      "epoch: 430, loss: 0.2510\n",
      "epoch: 440, loss: 0.2522\n",
      "epoch: 450, loss: 0.2513\n",
      "epoch: 460, loss: 0.2518\n",
      "epoch: 470, loss: 0.2500\n",
      "epoch: 480, loss: 0.3326\n",
      "epoch: 490, loss: 0.2379\n",
      "density model fitting done.\n",
      "evaluate genmodel\n",
      "marginal coverage of density model is 0.8490656444657403\n",
      "Average conditional coverage of density model is 0.7777777777777778\n",
      "wsc of density model is 0.776536312849162\n",
      "marginal coverage of density model is 0.729276473406804\n",
      "Average conditional coverage of density model is 0.6666666666666666\n",
      "wsc of density model is 0.7111111111111111\n",
      "marginal coverage of density model is 0.5999041686631529\n",
      "Average conditional coverage of density model is 0.5566820276497696\n",
      "wsc of density model is 0.4827586206896552\n",
      "marginal coverage of density model is 0.5117393387637758\n",
      "Average conditional coverage of density model is 0.4782608695652174\n",
      "wsc of density model is 0.38271604938271603\n",
      "marginal coverage of density model is 0.43267848586487784\n",
      "Average conditional coverage of density model is 0.391304347826087\n",
      "wsc of density model is 0.3271604938271605\n",
      "marginal coverage of density model is 0.35888835649257306\n",
      "Average conditional coverage of density model is 0.2962962962962963\n",
      "wsc of density model is 0.3724137931034483\n",
      "marginal coverage of density model is 0.2870148538572113\n",
      "Average conditional coverage of density model is 0.21739130434782608\n",
      "wsc of density model is 0.29411764705882354\n",
      "marginal coverage of density model is 0.2146621945376138\n",
      "Average conditional coverage of density model is 0.13043478260869565\n",
      "wsc of density model is 0.24742268041237114\n",
      "marginal coverage of density model is 0.14374700527072354\n",
      "Average conditional coverage of density model is 0.07407407407407407\n",
      "wsc of density model is 0.15668202764976957\n",
      "marginal coverage of density model is 0.08433157642549113\n",
      "Average conditional coverage of density model is 0.037037037037037035\n",
      "wsc of density model is 0.06806282722513089\n",
      "evaluate genmodel done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_104074/331148753.py:106: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  mdn_results_mean = mdn_results.groupby(['alpha']).mean().reset_index()\n",
      "/tmp/ipykernel_104074/331148753.py:107: FutureWarning: The default value of numeric_only in DataFrameGroupBy.std is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  mdn_results_se = mdn_results.groupby(['alpha']).std().reset_index() / np.sqrt(5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 0.563853919506073\n",
      "199 0.50897616147995\n",
      "299 0.47220614552497864\n",
      "399 0.44106656312942505\n",
      "499 0.41305670142173767\n",
      "599 0.3930303156375885\n",
      "699 0.3775741159915924\n",
      "799 0.36474114656448364\n",
      "899 0.35359087586402893\n",
      "999 0.3456641435623169\n",
      "1099 0.33952006697654724\n",
      "1199 0.33527156710624695\n",
      "1299 0.3314869999885559\n",
      "1399 0.32842782139778137\n",
      "1499 0.3262264132499695\n",
      "1599 0.3243827819824219\n",
      "1699 0.3223129212856293\n",
      "1799 0.3209473490715027\n",
      "1899 0.31955626606941223\n",
      "1999 0.31765642762184143\n",
      "2099 0.31645530462265015\n",
      "2199 0.31551143527030945\n",
      "2299 0.314611554145813\n",
      "2399 0.31338655948638916\n",
      "2499 0.31208786368370056\n",
      "2599 0.31100520491600037\n",
      "2699 0.30977195501327515\n",
      "2799 0.3085612654685974\n",
      "2899 0.30770212411880493\n",
      "2999 0.3068595826625824\n",
      "3099 0.30605289340019226\n",
      "3199 0.3051958680152893\n",
      "3299 0.3046673834323883\n",
      "3399 0.30421045422554016\n",
      "3499 0.30373501777648926\n",
      "3599 0.3032630383968353\n",
      "3699 0.3029496371746063\n",
      "3799 0.30262526869773865\n",
      "3899 0.3022840917110443\n",
      "3999 0.301984041929245\n",
      "4099 0.3014705181121826\n",
      "4199 0.3012290298938751\n",
      "4299 0.30097997188568115\n",
      "4399 0.3007363975048065\n",
      "4499 0.300603985786438\n",
      "4599 0.3004431426525116\n",
      "4699 0.30028730630874634\n",
      "4799 0.300140380859375\n",
      "4899 0.29997798800468445\n",
      "4999 0.29975825548171997\n",
      "5099 0.29945269227027893\n",
      "5199 0.2991422414779663\n",
      "5299 0.2989758849143982\n",
      "5399 0.2986467778682709\n",
      "5499 0.29839885234832764\n",
      "5599 0.29802677035331726\n",
      "5699 0.2979580760002136\n",
      "5799 0.2976979911327362\n",
      "5899 0.2976371943950653\n",
      "5999 0.297614187002182\n",
      "6099 0.29741349816322327\n",
      "6199 0.29742181301116943\n",
      "6299 0.2973107099533081\n",
      "6399 0.29722440242767334\n",
      "6499 0.29719939827919006\n",
      "6599 0.2971477508544922\n",
      "6699 0.2971476912498474\n",
      "6799 0.2971748411655426\n",
      "6899 0.2971939444541931\n",
      "6999 0.29724711179733276\n",
      "7099 0.2971094250679016\n",
      "7199 0.29717156291007996\n",
      "7299 0.29707691073417664\n",
      "7399 0.2970917820930481\n",
      "7499 0.29719278216362\n",
      "7599 0.2970764935016632\n",
      "7699 0.297084242105484\n",
      "7799 0.2970750629901886\n",
      "7899 0.29708385467529297\n",
      "7999 0.29705002903938293\n",
      "8099 0.2970048487186432\n",
      "8199 0.29697272181510925\n",
      "8299 0.29691410064697266\n",
      "8399 0.29679858684539795\n",
      "8499 0.296724408864975\n",
      "8599 0.29663196206092834\n",
      "8699 0.2965002655982971\n",
      "8799 0.2963848114013672\n",
      "8899 0.296347975730896\n",
      "8999 0.296316921710968\n",
      "9099 0.2963063418865204\n",
      "9199 0.29628846049308777\n",
      "9299 0.2962891459465027\n",
      "9399 0.2960799038410187\n",
      "9499 0.29594147205352783\n",
      "9599 0.2956385910511017\n",
      "9699 0.29553914070129395\n",
      "9799 0.2954629063606262\n",
      "9899 0.29547977447509766\n",
      "9999 0.2953038811683655\n",
      "cutoff is 1.7070093154907227\n",
      "Coverage is 0.9147101102060373\n",
      "Average coverage set size is 3.4140186309814453\n",
      "Average conditional coverage is 0.8518518518518519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_104074/331148753.py:118: DeprecationWarning: the `interpolation=` argument to quantile was renamed to `method=`, which has additional options.\n",
      "Users of the modes 'nearest', 'lower', 'higher', or 'midpoint' are encouraged to review the method they used. (Deprecated NumPy 1.22)\n",
      "  qhat = np.quantile(Yscore, np.ceil((nval+1)*(1-alpha))/nval, interpolation='higher')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wsc is 0.7319587628865979\n",
      "cutoff is 1.2247447967529297\n",
      "Coverage is 0.8217537134643028\n",
      "Average coverage set size is 2.4494895935058594\n",
      "Average conditional coverage is 0.6296296296296297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_104074/331148753.py:156: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  base_results_mean = base_results.groupby(['alpha']).mean().reset_index()\n",
      "/tmp/ipykernel_104074/331148753.py:157: FutureWarning: The default value of numeric_only in DataFrameGroupBy.std is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  base_results_se = base_results.groupby(['alpha']).std().reset_index() / np.sqrt(5)\n",
      "/tmp/ipykernel_104074/331148753.py:118: DeprecationWarning: the `interpolation=` argument to quantile was renamed to `method=`, which has additional options.\n",
      "Users of the modes 'nearest', 'lower', 'higher', or 'midpoint' are encouraged to review the method they used. (Deprecated NumPy 1.22)\n",
      "  qhat = np.quantile(Yscore, np.ceil((nval+1)*(1-alpha))/nval, interpolation='higher')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wsc is 0.6918604651162791\n",
      "cutoff is 0.9605779051780701\n",
      "Coverage is 0.7489218974604696\n",
      "Average coverage set size is 1.9211559295654297\n",
      "Average conditional coverage is 0.5555555555555556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_104074/331148753.py:156: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  base_results_mean = base_results.groupby(['alpha']).mean().reset_index()\n",
      "/tmp/ipykernel_104074/331148753.py:157: FutureWarning: The default value of numeric_only in DataFrameGroupBy.std is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  base_results_se = base_results.groupby(['alpha']).std().reset_index() / np.sqrt(5)\n",
      "/tmp/ipykernel_104074/331148753.py:118: DeprecationWarning: the `interpolation=` argument to quantile was renamed to `method=`, which has additional options.\n",
      "Users of the modes 'nearest', 'lower', 'higher', or 'midpoint' are encouraged to review the method they used. (Deprecated NumPy 1.22)\n",
      "  qhat = np.quantile(Yscore, np.ceil((nval+1)*(1-alpha))/nval, interpolation='higher')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wsc is 0.6272727272727273\n",
      "cutoff is 0.7972753047943115\n",
      "Coverage is 0.6775275515093435\n",
      "Average coverage set size is 1.5945507287979126\n",
      "Average conditional coverage is 0.5185185185185185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_104074/331148753.py:156: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  base_results_mean = base_results.groupby(['alpha']).mean().reset_index()\n",
      "/tmp/ipykernel_104074/331148753.py:157: FutureWarning: The default value of numeric_only in DataFrameGroupBy.std is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  base_results_se = base_results.groupby(['alpha']).std().reset_index() / np.sqrt(5)\n",
      "/tmp/ipykernel_104074/331148753.py:118: DeprecationWarning: the `interpolation=` argument to quantile was renamed to `method=`, which has additional options.\n",
      "Users of the modes 'nearest', 'lower', 'higher', or 'midpoint' are encouraged to review the method they used. (Deprecated NumPy 1.22)\n",
      "  qhat = np.quantile(Yscore, np.ceil((nval+1)*(1-alpha))/nval, interpolation='higher')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wsc is 0.4647058823529412\n",
      "cutoff is 0.6468201875686646\n",
      "Coverage is 0.5754671777671299\n",
      "Average coverage set size is 1.293640375137329\n",
      "Average conditional coverage is 0.4782608695652174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_104074/331148753.py:156: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  base_results_mean = base_results.groupby(['alpha']).mean().reset_index()\n",
      "/tmp/ipykernel_104074/331148753.py:157: FutureWarning: The default value of numeric_only in DataFrameGroupBy.std is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  base_results_se = base_results.groupby(['alpha']).std().reset_index() / np.sqrt(5)\n",
      "/tmp/ipykernel_104074/331148753.py:118: DeprecationWarning: the `interpolation=` argument to quantile was renamed to `method=`, which has additional options.\n",
      "Users of the modes 'nearest', 'lower', 'higher', or 'midpoint' are encouraged to review the method they used. (Deprecated NumPy 1.22)\n",
      "  qhat = np.quantile(Yscore, np.ceil((nval+1)*(1-alpha))/nval, interpolation='higher')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wsc is 0.42105263157894735\n",
      "cutoff is 0.5205499529838562\n",
      "Coverage is 0.4633445136559655\n",
      "Average coverage set size is 1.0410999059677124\n",
      "Average conditional coverage is 0.2608695652173913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_104074/331148753.py:156: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  base_results_mean = base_results.groupby(['alpha']).mean().reset_index()\n",
      "/tmp/ipykernel_104074/331148753.py:157: FutureWarning: The default value of numeric_only in DataFrameGroupBy.std is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  base_results_se = base_results.groupby(['alpha']).std().reset_index() / np.sqrt(5)\n",
      "/tmp/ipykernel_104074/331148753.py:118: DeprecationWarning: the `interpolation=` argument to quantile was renamed to `method=`, which has additional options.\n",
      "Users of the modes 'nearest', 'lower', 'higher', or 'midpoint' are encouraged to review the method they used. (Deprecated NumPy 1.22)\n",
      "  qhat = np.quantile(Yscore, np.ceil((nval+1)*(1-alpha))/nval, interpolation='higher')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wsc is 0.34210526315789475\n",
      "cutoff is 0.41456666588783264\n",
      "Coverage is 0.3828461907043603\n",
      "Average coverage set size is 0.8291332721710205\n",
      "Average conditional coverage is 0.21739130434782608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_104074/331148753.py:156: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  base_results_mean = base_results.groupby(['alpha']).mean().reset_index()\n",
      "/tmp/ipykernel_104074/331148753.py:157: FutureWarning: The default value of numeric_only in DataFrameGroupBy.std is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  base_results_se = base_results.groupby(['alpha']).std().reset_index() / np.sqrt(5)\n",
      "/tmp/ipykernel_104074/331148753.py:118: DeprecationWarning: the `interpolation=` argument to quantile was renamed to `method=`, which has additional options.\n",
      "Users of the modes 'nearest', 'lower', 'higher', or 'midpoint' are encouraged to review the method they used. (Deprecated NumPy 1.22)\n",
      "  qhat = np.quantile(Yscore, np.ceil((nval+1)*(1-alpha))/nval, interpolation='higher')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wsc is 0.2556390977443609\n",
      "cutoff is 0.30641674995422363\n",
      "Coverage is 0.2788691902252036\n",
      "Average coverage set size is 0.6128334403038025\n",
      "Average conditional coverage is 0.17391304347826086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_104074/331148753.py:156: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  base_results_mean = base_results.groupby(['alpha']).mean().reset_index()\n",
      "/tmp/ipykernel_104074/331148753.py:157: FutureWarning: The default value of numeric_only in DataFrameGroupBy.std is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  base_results_se = base_results.groupby(['alpha']).std().reset_index() / np.sqrt(5)\n",
      "/tmp/ipykernel_104074/331148753.py:118: DeprecationWarning: the `interpolation=` argument to quantile was renamed to `method=`, which has additional options.\n",
      "Users of the modes 'nearest', 'lower', 'higher', or 'midpoint' are encouraged to review the method they used. (Deprecated NumPy 1.22)\n",
      "  qhat = np.quantile(Yscore, np.ceil((nval+1)*(1-alpha))/nval, interpolation='higher')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wsc is 0.15950920245398773\n",
      "cutoff is 0.2165381908416748\n",
      "Coverage is 0.1964542405366555\n",
      "Average coverage set size is 0.433076411485672\n",
      "Average conditional coverage is 0.08695652173913043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_104074/331148753.py:156: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  base_results_mean = base_results.groupby(['alpha']).mean().reset_index()\n",
      "/tmp/ipykernel_104074/331148753.py:157: FutureWarning: The default value of numeric_only in DataFrameGroupBy.std is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  base_results_se = base_results.groupby(['alpha']).std().reset_index() / np.sqrt(5)\n",
      "/tmp/ipykernel_104074/331148753.py:118: DeprecationWarning: the `interpolation=` argument to quantile was renamed to `method=`, which has additional options.\n",
      "Users of the modes 'nearest', 'lower', 'higher', or 'midpoint' are encouraged to review the method they used. (Deprecated NumPy 1.22)\n",
      "  qhat = np.quantile(Yscore, np.ceil((nval+1)*(1-alpha))/nval, interpolation='higher')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wsc is 0.1388888888888889\n",
      "cutoff is 0.11733201146125793\n",
      "Coverage is 0.10637278390033542\n",
      "Average coverage set size is 0.23466400802135468\n",
      "Average conditional coverage is 0.043478260869565216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_104074/331148753.py:156: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  base_results_mean = base_results.groupby(['alpha']).mean().reset_index()\n",
      "/tmp/ipykernel_104074/331148753.py:157: FutureWarning: The default value of numeric_only in DataFrameGroupBy.std is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  base_results_se = base_results.groupby(['alpha']).std().reset_index() / np.sqrt(5)\n",
      "/tmp/ipykernel_104074/331148753.py:118: DeprecationWarning: the `interpolation=` argument to quantile was renamed to `method=`, which has additional options.\n",
      "Users of the modes 'nearest', 'lower', 'higher', or 'midpoint' are encouraged to review the method they used. (Deprecated NumPy 1.22)\n",
      "  qhat = np.quantile(Yscore, np.ceil((nval+1)*(1-alpha))/nval, interpolation='higher')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wsc is 0.12234042553191489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_104074/331148753.py:156: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  base_results_mean = base_results.groupby(['alpha']).mean().reset_index()\n",
      "/tmp/ipykernel_104074/331148753.py:157: FutureWarning: The default value of numeric_only in DataFrameGroupBy.std is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  base_results_se = base_results.groupby(['alpha']).std().reset_index() / np.sqrt(5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t =  0 loss =  tensor(0.2982, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.8108, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(81.3828, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  1 loss =  tensor(0.4157, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.7853, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(78.9488, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  2 loss =  tensor(0.6416, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.8174, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(82.3799, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  3 loss =  tensor(0.9612, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.7751, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(78.4746, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  4 loss =  tensor(1.4668, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.7603, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(77.4975, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  5 loss =  tensor(2.2364, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.6654, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(68.7762, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  6 loss =  tensor(3.0269, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.6576, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(68.7858, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  7 loss =  tensor(3.5226, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.6688, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(70.4020, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  8 loss =  tensor(3.7663, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.6387, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(67.6348, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  9 loss =  tensor(3.7479, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.6987, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(73.6146, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  10 loss =  tensor(3.6791, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.6589, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(69.5683, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  11 loss =  tensor(3.4523, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.6410, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(67.5571, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  12 loss =  tensor(3.3127, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.6119, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(64.5021, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  13 loss =  tensor(3.1049, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.7891, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(82.0119, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  14 loss =  tensor(3.1186, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.6830, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(71.4161, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  15 loss =  tensor(3.3439, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.5966, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(63.0016, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  16 loss =  tensor(3.5239, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.6953, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(73.0578, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  17 loss =  tensor(3.8698, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.6301, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(66.8759, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  18 loss =  tensor(4.4469, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.5982, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(64.2714, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  19 loss =  tensor(4.9251, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.6194, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(66.8671, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_104074/1767092640.py:253: DeprecationWarning: the `interpolation=` argument to quantile was renamed to `method=`, which has additional options.\n",
      "Users of the modes 'nearest', 'lower', 'higher', or 'midpoint' are encouraged to review the method they used. (Deprecated NumPy 1.22)\n",
      "  qhat = np.quantile(Yscore, np.ceil((nval+1)*(1-alpha))/nval, interpolation='higher')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cutoff is 3.604297399520874\n",
      "19 4.925108909606934\n",
      "mc is 0.8845232390991854\n",
      "cc is 0.8571428571428571\n",
      "wsc is 0.8089171974522293\n",
      "t =  20 loss =  tensor(5.2640, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.6063, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(65.8958, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  21 loss =  tensor(5.4619, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.5966, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(65.1256, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  22 loss =  tensor(5.4606, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.5872, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(64.1844, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  23 loss =  tensor(5.2962, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.5637, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(61.6681, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  24 loss =  tensor(4.9203, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.5474, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(59.6588, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  25 loss =  tensor(4.4260, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.5578, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(60.2065, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  26 loss =  tensor(3.9065, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.5216, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(56.0660, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  27 loss =  tensor(3.4552, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.5077, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(54.2285, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  28 loss =  tensor(3.0996, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.5247, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(55.5670, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  29 loss =  tensor(2.8340, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.5895, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(61.7841, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  30 loss =  tensor(2.7160, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.5471, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(57.4260, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  31 loss =  tensor(2.8092, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.4718, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(49.9928, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  32 loss =  tensor(2.8819, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.6020, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(63.0796, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  33 loss =  tensor(3.3573, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.4853, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(51.8910, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  34 loss =  tensor(3.7881, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.4803, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(51.8188, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  35 loss =  tensor(4.0465, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.4919, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(53.2375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  36 loss =  tensor(4.1638, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.5492, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(59.0827, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  37 loss =  tensor(4.2573, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.4916, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(53.4154, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  38 loss =  tensor(4.1963, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.4753, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(51.7220, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  39 loss =  tensor(4.0618, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.4835, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(52.4161, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_104074/1767092640.py:253: DeprecationWarning: the `interpolation=` argument to quantile was renamed to `method=`, which has additional options.\n",
      "Users of the modes 'nearest', 'lower', 'higher', or 'midpoint' are encouraged to review the method they used. (Deprecated NumPy 1.22)\n",
      "  qhat = np.quantile(Yscore, np.ceil((nval+1)*(1-alpha))/nval, interpolation='higher')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cutoff is 3.1130709648132324\n",
      "39 4.061784267425537\n",
      "mc is 0.8979396262577863\n",
      "cc is 0.8746543778801843\n",
      "wsc is 0.8525345622119815\n",
      "t =  40 loss =  tensor(3.8611, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.4571, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(49.5748, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  41 loss =  tensor(3.5862, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.4508, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(48.6648, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  42 loss =  tensor(3.2782, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.6935, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(72.6291, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  43 loss =  tensor(3.0552, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.4578, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(48.8377, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  44 loss =  tensor(2.8472, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.4392, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(46.7682, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  45 loss =  tensor(2.6123, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.4287, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(45.4862, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  46 loss =  tensor(2.4146, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.4214, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(44.5555, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  47 loss =  tensor(2.2445, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.3925, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(41.4916, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  48 loss =  tensor(2.0792, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.4113, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(43.2102, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  49 loss =  tensor(1.9289, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.4303, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(44.9625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  50 loss =  tensor(1.8824, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.3712, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(38.9974, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  51 loss =  tensor(1.9738, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.3495, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(36.9219, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  52 loss =  tensor(2.0333, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.4813, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(50.1648, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  53 loss =  tensor(1.9958, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.3990, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(41.9003, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  54 loss =  tensor(1.9854, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.3601, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(37.9949, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  55 loss =  tensor(1.9147, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.6103, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(62.9459, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  56 loss =  tensor(2.0677, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.4409, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(46.1562, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  57 loss =  tensor(2.4367, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.5658, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(59.0121, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  58 loss =  tensor(2.9762, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.4382, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(46.7913, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "t =  59 loss =  tensor(3.3442, device='cuda:0', grad_fn=<MeanBackward0>) div =  tensor(0.4834, device='cuda:0', grad_fn=<SumBackward0>) loss1 =  tensor(51.6868, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_104074/1767092640.py:253: DeprecationWarning: the `interpolation=` argument to quantile was renamed to `method=`, which has additional options.\n",
      "Users of the modes 'nearest', 'lower', 'higher', or 'midpoint' are encouraged to review the method they used. (Deprecated NumPy 1.22)\n",
      "  qhat = np.quantile(Yscore, np.ceil((nval+1)*(1-alpha))/nval, interpolation='higher')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cutoff is 2.812591552734375\n",
      "59 3.344215154647827\n",
      "mc is 0.8941063727839004\n",
      "cc is 0.8820276497695853\n",
      "wsc is 0.8561643835616438\n",
      "cutoff is 2.3128604888916016\n",
      "Coverage is 0.900335409678965\n",
      "Average coverage set size is 4.625720977783203\n",
      "Average conditional coverage is 0.8518518518518519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_104074/331148753.py:179: DeprecationWarning: the `interpolation=` argument to quantile was renamed to `method=`, which has additional options.\n",
      "Users of the modes 'nearest', 'lower', 'higher', or 'midpoint' are encouraged to review the method they used. (Deprecated NumPy 1.22)\n",
      "  qhat = np.quantile(Yscore, np.ceil((nval+1)*(1-alpha))/nval, interpolation='higher')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wsc is 0.9075144508670521\n",
      "WSLAB is 0.9075144508670521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_104074/331148753.py:216: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  result_mean = results.groupby(['alpha']).mean().reset_index()\n",
      "/tmp/ipykernel_104074/331148753.py:217: FutureWarning: The default value of numeric_only in DataFrameGroupBy.std is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  result_se = results.groupby(['alpha']).std().reset_index() / np.sqrt(5)\n"
     ]
    }
   ],
   "source": [
    "# mlp cc model \n",
    "seed_everything(0)\n",
    "X, Y = GetDataset(dataset_name, './data/')\n",
    "Y += 1e-3 * np.random.normal(size=Y.shape)\n",
    "\n",
    "args.model = 'mlp_cc'\n",
    "model = 'mlp_cc'\n",
    "basemodel = TorchBaseModel(X.shape[1])\n",
    "\n",
    "index = np.random.permutation(X.shape[0])\n",
    "print(f'dataset size is {X.shape[0]}, number of features is {X.shape[1]}.')\n",
    "nval = int(X.shape[0] * 0.2)\n",
    "ntrain = int(X.shape[0] * 0.6)\n",
    "Xval = X[index[:nval],:]\n",
    "Yval = Y[index[:nval]]\n",
    "Xtest = X[index[(nval+ntrain):],:]\n",
    "Ytest = Y[index[(nval+ntrain):]]\n",
    "X = X[index[(nval):(nval+ntrain)],:]\n",
    "Y = Y[index[(nval):(nval+ntrain)]]\n",
    "print(f'train size is {X.shape[0]}, val size is {Xval.shape[0]}, test size is {Xtest.shape[0]}.')\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "\n",
    "y_scaler = StandardScaler()\n",
    "y_scaler.fit(Y.reshape(-1,1))\n",
    "\n",
    "Y = y_scaler.transform(Y.reshape(-1,1)).reshape(-1)\n",
    "Yval = y_scaler.transform(Yval.reshape(-1,1)).reshape(-1)\n",
    "Ytest = y_scaler.transform(Ytest.reshape(-1,1)).reshape(-1)\n",
    "\n",
    "if dataset_name == 'meps_19' or dataset_name == 'meps_20' or dataset_name == 'meps_21':\n",
    "    # define a subgroup - checking some surrogate variable\n",
    "    sen = np.zeros_like(Ytest)\n",
    "    sen[(Xtest[:, 9] == 1) & (Xtest[:, -1] == 1)] = 0\n",
    "    sen[(Xtest[:, 9] == 1) & (Xtest[:, -1] == 0)] = 1\n",
    "    sen[(Xtest[:, 9] == 0) & (Xtest[:, -1] == 1)] = 2\n",
    "    sen[(Xtest[:, 9] == 0) & (Xtest[:, -1] == 0)] = 3\n",
    "    \n",
    "X = scaler.transform(X)\n",
    "Xval = scaler.transform(Xval)\n",
    "Xtest = scaler.transform(Xtest)\n",
    "\n",
    "if (model == 'linear' or model == 'mlp') and args.conformalscore == 'residual':\n",
    "    pass \n",
    "elif args.densitymodel == 'CVAE':\n",
    "    density_model = CVAE(n_epochs=500, latent_dim=8, hidden = (32,32), batch_size=128, lr=1e-3, weight_decay=1e-4, verbose = 1)\n",
    "    density_model.fit(Y.reshape(-1,1), X)\n",
    "\n",
    "print('density model fitting done.')\n",
    "nval = Xval.shape[0]\n",
    "\n",
    "nsamples = 1000\n",
    "\n",
    "if (model == 'linear' or model == 'mlp') and args.conformalscore == 'residual':\n",
    "    val_std = 1\n",
    "    test_std = 1\n",
    "else:\n",
    "    samples_val = np.zeros((nval, nsamples))\n",
    "    for i in range(nsamples):\n",
    "        #samples_val[:,i] = density_model.sample(Xval)[1].reshape(-1)\n",
    "        #samples_val[:,i] = density_model.generate(Xval.shape[0], cond = Xval).values.reshape(-1)\n",
    "        samples_val[:,i] = density_model.sample(Xval).reshape(-1)\n",
    "\n",
    "    val_std = np.std(samples_val, axis = 1)\n",
    "\n",
    "    ntest = Xtest.shape[0]\n",
    "    samples = np.zeros((ntest, nsamples))\n",
    "    for i in range(nsamples):\n",
    "        #samples[:,i] = density_model.sample(Xtest)[1].reshape(-1)\n",
    "        #samples[:,i] = density_model.generate(Xtest.shape[0], cond = Xtest).values.reshape(-1)\n",
    "        samples[:,i] = density_model.sample(Xtest).reshape(-1)\n",
    "\n",
    "    test_std = np.std(samples, axis = 1)\n",
    "\n",
    "    # evaluate genmodel \n",
    "    print('evaluate genmodel')\n",
    "    for alpha in alpha_grid:\n",
    "        # evaluate density model's conditional/marginal coverage\n",
    "        Y_lo = np.quantile(samples, alpha/2, axis = 1)\n",
    "        Y_up = np.quantile(samples, 1 - alpha/2, axis = 1)\n",
    "\n",
    "        marginal_coverage = np.mean((Y_lo <= Ytest) & (Ytest <= Y_up))\n",
    "        print(f'marginal coverage of density model is {marginal_coverage}')\n",
    "\n",
    "        cover = (Y_lo <= Ytest) & (Ytest <= Y_up)\n",
    "        cc = 1\n",
    "        for eachs in set(sen):\n",
    "            cc = min(cc, cover[sen == eachs].mean())\n",
    "\n",
    "        conditional_coverage.append(cc)\n",
    "        print(f'Average conditional coverage of density model is {cc}')\n",
    "        coverage.append(np.mean((Y_lo <= Ytest) & (Ytest <= Y_up)))\n",
    "        set_size.append(np.mean(Y_up - Y_lo))\n",
    "\n",
    "        pred = np.concatenate([Y_lo.reshape(-1,1), Y_up.reshape(-1,1)], axis = 1)\n",
    "        wslab = wsc_unbiased(Xtest, Ytest, pred, delta=args.wsc_delta, M=max(200,Xtest.shape[1]+50), test_size=0.5, random_state=2020, verbose=False) \n",
    "        print(f'wsc of density model is {wslab}')\n",
    "        est_mean = np.mean(samples, axis = 1)\n",
    "\n",
    "        mdn_results = pd.concat([mdn_results, pd.DataFrame({\"iter\": [iter], \"alpha\": [alpha], \"coverage\": [np.mean((Y_lo <= Ytest) & (Ytest <= Y_up))], \\\n",
    "        \"set_size\": [np.mean(Y_up - Y_lo)], \"conditional_coverage\": [np.min(cc)], 'mse': [((Ytest - est_mean)**2).mean()], \n",
    "        'wslab': [wslab]})])\n",
    "    print('evaluate genmodel done.')\n",
    "\n",
    "    mdn_results.to_csv(f'./log/{args.dataset}_{args.densitymodel}_genmodel_coverage.csv', index=False)\n",
    "    mdn_results_mean = mdn_results.groupby(['alpha']).mean().reset_index()\n",
    "    mdn_results_se = mdn_results.groupby(['alpha']).std().reset_index() / np.sqrt(5)\n",
    "    mdn_results_se.columns = [each + '_se' for each in mdn_results_se.columns]\n",
    "    mdn_results_sum = pd.concat([mdn_results_mean, mdn_results_se], axis = 1)\n",
    "    mdn_results_sum.to_csv(f'./log/{args.dataset}_{args.densitymodel}_genmodel_summary.csv', index=False)\n",
    "\n",
    "    # evaluate basemodel \n",
    "    Ymodel_base = basemodel.fit(X, Y)\n",
    "    Yhat = Ymodel_base.predict(Xval)\n",
    "    Yscore = conformalScore(Yval, Yhat, sd = val_std)\n",
    "    Yhat_test = Ymodel_base.predict(Xtest)\n",
    "    for alpha in alpha_grid:\n",
    "        qhat = np.quantile(Yscore, np.ceil((nval+1)*(1-alpha))/nval, interpolation='higher')\n",
    "        Yslack = qhat\n",
    "\n",
    "        print(f'cutoff is {Yslack}')\n",
    "        if args.conformalscore == 'residual':\n",
    "            Ylo = Yhat_test - Yslack\n",
    "            Yup = Yhat_test + Yslack\n",
    "        elif args.conformalscore == 'normalized':\n",
    "            Ylo = Yhat_test - Yslack * test_std\n",
    "            Yup = Yhat_test + Yslack * test_std\n",
    "\n",
    "        CI = pd.DataFrame({\"lower\": Ylo.reshape(-1), \"upper\": Yup.reshape(-1)})\n",
    "\n",
    "        # check subgroup coverage\n",
    "        cover = (Ylo <= Ytest) & (Ytest <= Yup)\n",
    "        cc = 1\n",
    "        for eachs in set(sen):\n",
    "            cc = min(cc, cover[sen == eachs].mean())\n",
    "\n",
    "        conditional_coverage.append(cc)\n",
    "        print(f'Coverage is {np.mean((Ylo <= Ytest) & (Ytest <= Yup))}')\n",
    "        print(f'Average coverage set size is {np.mean(Yup - Ylo)}')\n",
    "        print(f'Average conditional coverage is {cc}')\n",
    "        coverage.append(np.mean((Ylo <= Ytest) & (Ytest <= Yup)))\n",
    "        set_size.append(np.mean(Yup - Ylo))\n",
    "\n",
    "        pred = np.concatenate([Ylo.reshape(-1,1), Yup.reshape(-1,1)], axis = 1)\n",
    "        wslab = wsc_unbiased(Xtest, Ytest, pred, delta=args.wsc_delta, M=max(200,Xtest.shape[1]+50), test_size=0.5, random_state=2020, verbose=False)\n",
    "        print(f'wsc is {wslab}')\n",
    "\n",
    "        covered_index = ((Ylo <= Ytest) & (Ytest <= Yup))\n",
    "        base_results = pd.concat([base_results, pd.DataFrame({\"iter\": [iter], \"alpha\": [alpha], \"coverage\": [np.mean((Ylo <= Ytest) & (Ytest <= Yup))], \\\n",
    "        \"set_size\": [np.mean(Yup - Ylo)], \"conditional_coverage\": [np.min(cc)], 'mse': [((Ytest - Yhat_test)**2).mean()], \n",
    "        \"wslab\": [wslab]})])\n",
    "\n",
    "        filename = f'./log/{args.dataset}_{args.model}_{args.densitymodel}_{args.conformalscore}_base_coverage.csv'\n",
    "        filename = filename.replace('_cc', '')\n",
    "        base_results.to_csv(filename, index=False)\n",
    "        base_results_mean = base_results.groupby(['alpha']).mean().reset_index()\n",
    "        base_results_se = base_results.groupby(['alpha']).std().reset_index() / np.sqrt(5)\n",
    "        base_results_se.columns = [each + '_se' for each in base_results_se.columns]\n",
    "        base_results_sum = pd.concat([base_results_mean, base_results_se], axis = 1)\n",
    "        filename = f'./log/{args.dataset}_{args.model}_{args.densitymodel}_{args.conformalscore}_base_summary.csv'\n",
    "        filename = filename.replace('_cc', '')\n",
    "        base_results_sum.to_csv(filename, index=False)\n",
    "\n",
    "if model == 'linear_cc':\n",
    "    linearmodel_cc = TorchLinearModel_CC(X.shape[1], lamb=args.lamb, density_model = density_model)\n",
    "    Ymodel = linearmodel_cc.fit(X, Y, Xval, Yval, Xtest, Ytest, sen, i = iter)\n",
    "elif model == 'mlp_cc':\n",
    "    linearmodel_cc = TorchLinearModel_CC(X.shape[1], lamb=args.lamb, density_model = density_model)\n",
    "    Ymodel = linearmodel_cc.fit(X, Y, Xval, Yval, Xtest, Ytest, sen, i = iter)\n",
    "elif model == 'linear' or model == 'mlp':\n",
    "    linearmodel = TorchBaseModel(X.shape[1])\n",
    "    Ymodel = linearmodel.fit(X, Y)\n",
    "\n",
    "Yhat = Ymodel.predict(Xval)\n",
    "Yscore = conformalScore(Yval, Yhat, sd = val_std)\n",
    "Yhat_test = Ymodel.predict(Xtest)\n",
    "\n",
    "alpha = 0.1 \n",
    "qhat = np.quantile(Yscore, np.ceil((nval+1)*(1-alpha))/nval, interpolation='higher')\n",
    "Yslack = qhat\n",
    "\n",
    "print(f'cutoff is {Yslack}')\n",
    "if args.conformalscore == 'residual':\n",
    "    Ylo = Yhat_test - Yslack\n",
    "    Yup = Yhat_test + Yslack\n",
    "elif args.conformalscore == 'normalized':\n",
    "    Ylo = Yhat_test - Yslack * test_std\n",
    "    Yup = Yhat_test + Yslack * test_std\n",
    "\n",
    "CI = pd.DataFrame({\"lower\": Ylo.reshape(-1), \"upper\": Yup.reshape(-1)})\n",
    "\n",
    "# check subgroup coverage\n",
    "cover = (Ylo <= Ytest) & (Ytest <= Yup)\n",
    "cc = 1\n",
    "for eachs in set(sen):\n",
    "    cc = min(cc, cover[sen == eachs].mean())\n",
    "\n",
    "conditional_coverage.append(cc)\n",
    "print(f'Coverage is {np.mean((Ylo <= Ytest) & (Ytest <= Yup))}')\n",
    "print(f'Average coverage set size is {np.mean(Yup - Ylo)}')\n",
    "print(f'Average conditional coverage is {cc}')\n",
    "coverage.append(np.mean((Ylo <= Ytest) & (Ytest <= Yup)))\n",
    "set_size.append(np.mean(Yup - Ylo))\n",
    "\n",
    "pred = np.concatenate([Ylo.reshape(-1,1), Yup.reshape(-1,1)], axis = 1)\n",
    "wslab = wsc_unbiased(Xtest, Ytest, pred, delta=0.1, M=max(200,Xtest.shape[1]+50), test_size=0.5, random_state=2020, verbose=False)\n",
    "print(f'wsc is {wslab}')\n",
    "\n",
    "covered_index = ((Ylo <= Ytest) & (Ytest <= Yup))\n",
    "results = pd.concat([results, pd.DataFrame({\"iter\": [iter], \"alpha\": [alpha], \"coverage\": [np.mean((Ylo <= Ytest) & (Ytest <= Yup))], \\\n",
    "\"set_size\": [np.mean(Yup - Ylo)], \"conditional_coverage\": [np.min(cc)], 'mse': [((Ytest - Yhat_test)**2).mean()], \n",
    "\"wslab\": [wslab]})])\n",
    "\n",
    "results.to_csv(f'./log/{args.dataset}_{args.model}_{args.lamb}_{args.densitymodel}_{args.conformalscore}_coverage.csv', index=False)\n",
    "\n",
    "result_mean = results.groupby(['alpha']).mean().reset_index()\n",
    "result_se = results.groupby(['alpha']).std().reset_index() / np.sqrt(5)\n",
    "result_se.columns = [each + '_se' for each in result_se.columns]\n",
    "result_sum = pd.concat([result_mean, result_se], axis = 1)\n",
    "result_sum.to_csv(f'./log/{args.dataset}_{args.model}_{args.lamb}_{args.densitymodel}_{args.conformalscore}_coverage_summary.csv', index=False)\n",
    "    \n",
    "print(f'WSLAB is {wslab}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WSLAB is 0.9075144508670521\n"
     ]
    }
   ],
   "source": [
    "print(f'WSLAB is {wslab}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdeenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
